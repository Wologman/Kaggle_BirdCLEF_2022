{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Training Notebook for BirdCLEF2022 ##\n","\n","[Original baseline by Kaerururu](https://www.kaggle.com/code/kaerunantoka/birdclef2022-use-2nd-label-f0/notebook)  \n","[That was forked from Kidehisa Arai (2021 comp)](https://www.kaggle.com/code/hidehisaarai1213/pytorch-inference-birdclef2021-starter/notebook)  \n","[My inference notebook](https://www.kaggle.com/code/ollypowell/birdclef2022-ex005-f0-infer/)  \n","[Original Infernence fork](https://www.kaggle.com/code/kaerunantoka/birdclef2022-ex005-f0-infer/notebook)\n","\n","Data:  \n","https://www.kaggle.com/kaerunantoka/birdclef2022-audio-to-numpy-1-4  \n","https://www.kaggle.com/kaerunantoka/birdclef2022-audio-to-numpy-2-4  \n","https://www.kaggle.com/kaerunantoka/birdclef2022-audio-to-numpy-3-4  \n","https://www.kaggle.com/kaerunantoka/birdclef2022-audio-to-numpy-4-4  \n","\n","\n","**My Strategy:**  \n","1. Set up the training notebook on my own GPU, so no time limits\n","2. Run with all folds, while I work on next step.  Run inference on Kaggle once the models are ready.\n","3. Improve my CV score as much as I can by doing data augmentation, starting with the basic strategies below from Shinmaru, use 128 image size to speed up if need be.  \n","2. Train on additional folds\n","3. Fine tune the inference threshold (Should be in the vacinity of .005 to 0.1)\n","\n","[**Basic Augmentation strategies, suggested to be useful by Shinmaru:**](https://www.kaggle.com/competitions/birdclef-2022/discussion/324318)\n","\n","* Time shift\n","* Add pink noise and brown noise\n","* Mix other audio dataset\n","\n","[Good notebook on this topic by Hidehisa Arai](https://www.kaggle.com/code/hidehisaarai1213/rfcx-audio-data-augmentation-japanese-english)  \n","[Time and noise only covered by Shinmaru](https://www.kaggle.com/code/shinmurashinmura/birdclef2022-basic-augmentation/notebook)\n","\n","[**More advanced ideas (also suggested to work from Shinmaru)**](https://www.kaggle.com/competitions/birdclef-2022/discussion/307880)  \n","\n","[SpecAugment](https://arxiv.org/abs/1904.08779)  \n","[SpecAugment++](https://arxiv.org/abs/2103.16858v3)  \n","[ImportantAug](https://arxiv.org/abs/2112.07156)"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import sys\n","sys.path.append('input/pytorch-image-models/pytorch-image-models-master')  # removed ../\n","import random\n","import time\n","import librosa\n","import colorednoise as cn\n","import numpy as np\n","import pandas as pd\n","import timm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn import metrics\n","from torchlibrosa.augmentation import SpecAugmentation\n","from tqdm import tqdm\n","import ast\n","import glob \n","import albumentations as A\n","import audiomentations as AA\n","import transformers\n","from torch.cuda.amp import autocast, GradScaler"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Setup complete. Using torch 1.11.0 _CudaDeviceProperties(name='NVIDIA GeForce RTX 3060 Laptop GPU', major=8, minor=6, total_memory=5946MB, multi_processor_count=30)\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["14852"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Changed the paths to suit my own filenames\n","all_path = glob.glob('input/train_np_1/*/*.npy')\\\n","+ glob.glob('input/train_np_2/*/*.npy')\\\n","+ glob.glob('input/train_np_3/*/*.npy')\\\n","+ glob.glob('input/train_np_4/*/*.npy')\n","\n","len(all_path)"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>primary_label</th>\n","      <th>secondary_labels</th>\n","      <th>type</th>\n","      <th>latitude</th>\n","      <th>longitude</th>\n","      <th>scientific_name</th>\n","      <th>common_name</th>\n","      <th>author</th>\n","      <th>license</th>\n","      <th>rating</th>\n","      <th>time</th>\n","      <th>url</th>\n","      <th>filename</th>\n","      <th>new_target</th>\n","      <th>len_new_target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>afrsil1</td>\n","      <td>[]</td>\n","      <td>['call', 'flight call']</td>\n","      <td>12.3910</td>\n","      <td>-1.4930</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Bram Piot</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>2.5</td>\n","      <td>08:00</td>\n","      <td>https://www.xeno-canto.org/125458</td>\n","      <td>afrsil1/XC125458.ogg</td>\n","      <td>afrsil1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>afrsil1</td>\n","      <td>['houspa', 'redava', 'zebdov']</td>\n","      <td>['call']</td>\n","      <td>19.8801</td>\n","      <td>-155.7254</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Dan Lane</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>3.5</td>\n","      <td>08:30</td>\n","      <td>https://www.xeno-canto.org/175522</td>\n","      <td>afrsil1/XC175522.ogg</td>\n","      <td>afrsil1 houspa redava zebdov</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>afrsil1</td>\n","      <td>[]</td>\n","      <td>['call', 'song']</td>\n","      <td>16.2901</td>\n","      <td>-16.0321</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Bram Piot</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>4.0</td>\n","      <td>11:30</td>\n","      <td>https://www.xeno-canto.org/177993</td>\n","      <td>afrsil1/XC177993.ogg</td>\n","      <td>afrsil1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>afrsil1</td>\n","      <td>[]</td>\n","      <td>['alarm call', 'call']</td>\n","      <td>17.0922</td>\n","      <td>54.2958</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Oscar Campbell</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>4.0</td>\n","      <td>11:00</td>\n","      <td>https://www.xeno-canto.org/205893</td>\n","      <td>afrsil1/XC205893.ogg</td>\n","      <td>afrsil1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>afrsil1</td>\n","      <td>[]</td>\n","      <td>['flight call']</td>\n","      <td>21.4581</td>\n","      <td>-157.7252</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Ross Gallardy</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>3.0</td>\n","      <td>16:30</td>\n","      <td>https://www.xeno-canto.org/207431</td>\n","      <td>afrsil1/XC207431.ogg</td>\n","      <td>afrsil1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  primary_label                secondary_labels                     type  \\\n","0       afrsil1                              []  ['call', 'flight call']   \n","1       afrsil1  ['houspa', 'redava', 'zebdov']                 ['call']   \n","2       afrsil1                              []         ['call', 'song']   \n","3       afrsil1                              []   ['alarm call', 'call']   \n","4       afrsil1                              []          ['flight call']   \n","\n","   latitude  longitude  scientific_name         common_name          author  \\\n","0   12.3910    -1.4930  Euodice cantans  African Silverbill       Bram Piot   \n","1   19.8801  -155.7254  Euodice cantans  African Silverbill        Dan Lane   \n","2   16.2901   -16.0321  Euodice cantans  African Silverbill       Bram Piot   \n","3   17.0922    54.2958  Euodice cantans  African Silverbill  Oscar Campbell   \n","4   21.4581  -157.7252  Euodice cantans  African Silverbill   Ross Gallardy   \n","\n","                                             license  rating   time  \\\n","0  Creative Commons Attribution-NonCommercial-Sha...     2.5  08:00   \n","1  Creative Commons Attribution-NonCommercial-Sha...     3.5  08:30   \n","2  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:30   \n","3  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:00   \n","4  Creative Commons Attribution-NonCommercial-Sha...     3.0  16:30   \n","\n","                                 url              filename  \\\n","0  https://www.xeno-canto.org/125458  afrsil1/XC125458.ogg   \n","1  https://www.xeno-canto.org/175522  afrsil1/XC175522.ogg   \n","2  https://www.xeno-canto.org/177993  afrsil1/XC177993.ogg   \n","3  https://www.xeno-canto.org/205893  afrsil1/XC205893.ogg   \n","4  https://www.xeno-canto.org/207431  afrsil1/XC207431.ogg   \n","\n","                     new_target  len_new_target  \n","0                      afrsil1                1  \n","1  afrsil1 houspa redava zebdov               4  \n","2                      afrsil1                1  \n","3                      afrsil1                1  \n","4                      afrsil1                1  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv('input/birdclef-2022/train_metadata.csv')  #Changed filepath to suit\n","\n","train['new_target'] = train['primary_label'] + ' ' + train['secondary_labels'].map(lambda x: ' '.join(ast.literal_eval(x)))\n","train['len_new_target'] = train['new_target'].map(lambda x: len(x.split()))\n","# train['len_new_target'].value_counts()\n","train.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>file_path</th>\n","      <th>filename</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>input/train_np_1/bcnher/XC256938.ogg.npy</td>\n","      <td>bcnher/XC256938.ogg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>input/train_np_1/bcnher/XC648367.ogg.npy</td>\n","      <td>bcnher/XC648367.ogg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>input/train_np_1/bcnher/XC587839.ogg.npy</td>\n","      <td>bcnher/XC587839.ogg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>input/train_np_1/bcnher/XC548602.ogg.npy</td>\n","      <td>bcnher/XC548602.ogg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>input/train_np_1/bcnher/XC500284.ogg.npy</td>\n","      <td>bcnher/XC500284.ogg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                  file_path             filename\n","0  input/train_np_1/bcnher/XC256938.ogg.npy  bcnher/XC256938.ogg\n","1  input/train_np_1/bcnher/XC648367.ogg.npy  bcnher/XC648367.ogg\n","2  input/train_np_1/bcnher/XC587839.ogg.npy  bcnher/XC587839.ogg\n","3  input/train_np_1/bcnher/XC548602.ogg.npy  bcnher/XC548602.ogg\n","4  input/train_np_1/bcnher/XC500284.ogg.npy  bcnher/XC500284.ogg"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["path_df = pd.DataFrame(all_path, columns=['file_path'])\n","path_df['filename'] = path_df['file_path'].map(lambda x: x.split('/')[-2]+'/'+x.split('/')[-1][:-4])\n","path_df.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(14852, 16)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>primary_label</th>\n","      <th>secondary_labels</th>\n","      <th>type</th>\n","      <th>latitude</th>\n","      <th>longitude</th>\n","      <th>scientific_name</th>\n","      <th>common_name</th>\n","      <th>author</th>\n","      <th>license</th>\n","      <th>rating</th>\n","      <th>time</th>\n","      <th>url</th>\n","      <th>filename</th>\n","      <th>new_target</th>\n","      <th>len_new_target</th>\n","      <th>file_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>afrsil1</td>\n","      <td>[]</td>\n","      <td>['call', 'flight call']</td>\n","      <td>12.3910</td>\n","      <td>-1.4930</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Bram Piot</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>2.5</td>\n","      <td>08:00</td>\n","      <td>https://www.xeno-canto.org/125458</td>\n","      <td>afrsil1/XC125458.ogg</td>\n","      <td>afrsil1</td>\n","      <td>1</td>\n","      <td>input/train_np_1/afrsil1/XC125458.ogg.npy</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>afrsil1</td>\n","      <td>['houspa', 'redava', 'zebdov']</td>\n","      <td>['call']</td>\n","      <td>19.8801</td>\n","      <td>-155.7254</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Dan Lane</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>3.5</td>\n","      <td>08:30</td>\n","      <td>https://www.xeno-canto.org/175522</td>\n","      <td>afrsil1/XC175522.ogg</td>\n","      <td>afrsil1 houspa redava zebdov</td>\n","      <td>4</td>\n","      <td>input/train_np_1/afrsil1/XC175522.ogg.npy</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>afrsil1</td>\n","      <td>[]</td>\n","      <td>['call', 'song']</td>\n","      <td>16.2901</td>\n","      <td>-16.0321</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Bram Piot</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>4.0</td>\n","      <td>11:30</td>\n","      <td>https://www.xeno-canto.org/177993</td>\n","      <td>afrsil1/XC177993.ogg</td>\n","      <td>afrsil1</td>\n","      <td>1</td>\n","      <td>input/train_np_1/afrsil1/XC177993.ogg.npy</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>afrsil1</td>\n","      <td>[]</td>\n","      <td>['alarm call', 'call']</td>\n","      <td>17.0922</td>\n","      <td>54.2958</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Oscar Campbell</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>4.0</td>\n","      <td>11:00</td>\n","      <td>https://www.xeno-canto.org/205893</td>\n","      <td>afrsil1/XC205893.ogg</td>\n","      <td>afrsil1</td>\n","      <td>1</td>\n","      <td>input/train_np_1/afrsil1/XC205893.ogg.npy</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>afrsil1</td>\n","      <td>[]</td>\n","      <td>['flight call']</td>\n","      <td>21.4581</td>\n","      <td>-157.7252</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Ross Gallardy</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>3.0</td>\n","      <td>16:30</td>\n","      <td>https://www.xeno-canto.org/207431</td>\n","      <td>afrsil1/XC207431.ogg</td>\n","      <td>afrsil1</td>\n","      <td>1</td>\n","      <td>input/train_np_1/afrsil1/XC207431.ogg.npy</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  primary_label                secondary_labels                     type  \\\n","0       afrsil1                              []  ['call', 'flight call']   \n","1       afrsil1  ['houspa', 'redava', 'zebdov']                 ['call']   \n","2       afrsil1                              []         ['call', 'song']   \n","3       afrsil1                              []   ['alarm call', 'call']   \n","4       afrsil1                              []          ['flight call']   \n","\n","   latitude  longitude  scientific_name         common_name          author  \\\n","0   12.3910    -1.4930  Euodice cantans  African Silverbill       Bram Piot   \n","1   19.8801  -155.7254  Euodice cantans  African Silverbill        Dan Lane   \n","2   16.2901   -16.0321  Euodice cantans  African Silverbill       Bram Piot   \n","3   17.0922    54.2958  Euodice cantans  African Silverbill  Oscar Campbell   \n","4   21.4581  -157.7252  Euodice cantans  African Silverbill   Ross Gallardy   \n","\n","                                             license  rating   time  \\\n","0  Creative Commons Attribution-NonCommercial-Sha...     2.5  08:00   \n","1  Creative Commons Attribution-NonCommercial-Sha...     3.5  08:30   \n","2  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:30   \n","3  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:00   \n","4  Creative Commons Attribution-NonCommercial-Sha...     3.0  16:30   \n","\n","                                 url              filename  \\\n","0  https://www.xeno-canto.org/125458  afrsil1/XC125458.ogg   \n","1  https://www.xeno-canto.org/175522  afrsil1/XC175522.ogg   \n","2  https://www.xeno-canto.org/177993  afrsil1/XC177993.ogg   \n","3  https://www.xeno-canto.org/205893  afrsil1/XC205893.ogg   \n","4  https://www.xeno-canto.org/207431  afrsil1/XC207431.ogg   \n","\n","                     new_target  len_new_target  \\\n","0                      afrsil1                1   \n","1  afrsil1 houspa redava zebdov               4   \n","2                      afrsil1                1   \n","3                      afrsil1                1   \n","4                      afrsil1                1   \n","\n","                                   file_path  \n","0  input/train_np_1/afrsil1/XC125458.ogg.npy  \n","1  input/train_np_1/afrsil1/XC175522.ogg.npy  \n","2  input/train_np_1/afrsil1/XC177993.ogg.npy  \n","3  input/train_np_1/afrsil1/XC205893.ogg.npy  \n","4  input/train_np_1/afrsil1/XC207431.ogg.npy  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.merge(train, path_df, on='filename')\n","print(train.shape)\n","train.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n","  warnings.warn(\n"]}],"source":["Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","for n, (trn_index, val_index) in enumerate(Fold.split(train, train['primary_label'])):\n","    train.loc[val_index, 'kfold'] = int(n)\n","train['kfold'] = train['kfold'].astype(int)"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["train.to_csv('train_folds.csv', index=False)"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["class CFG:\n","    ######################\n","    # Globals #\n","    ######################\n","    EXP_ID = 'EX005'\n","    seed = 71\n","    epochs = 32     # Was 23\n","    cutmix_and_mixup_epochs = 18\n","    folds =  [0, 1, 2, 3, 4]  #[0]\n","    N_FOLDS = 5\n","    LR = 1e-3\n","    ETA_MIN = 1e-6\n","    WEIGHT_DECAY = 1e-6\n","    train_bs = 16 # 32\n","    valid_bs = 32 # 64\n","    base_model_name = \"tf_efficientnet_b0_ns\"\n","    EARLY_STOPPING = True\n","    DEBUG = False # True\n","    EVALUATION = 'AUC'\n","    apex = True\n","\n","    pooling = \"max\"\n","    pretrained = True\n","    num_classes = 152\n","    in_channels = 3\n","    target_columns = 'afrsil1 akekee akepa1 akiapo akikik amewig aniani apapan arcter \\\n","                      barpet bcnher belkin1 bkbplo bknsti bkwpet blkfra blknod bongul \\\n","                      brant brnboo brnnod brnowl brtcur bubsan buffle bulpet burpar buwtea \\\n","                      cacgoo1 calqua cangoo canvas caster1 categr chbsan chemun chukar cintea \\\n","                      comgal1 commyn compea comsan comwax coopet crehon dunlin elepai ercfra eurwig \\\n","                      fragul gadwal gamqua glwgul gnwtea golphe grbher3 grefri gresca gryfra gwfgoo \\\n","                      hawama hawcoo hawcre hawgoo hawhaw hawpet1 hoomer houfin houspa hudgod iiwi incter1 \\\n","                      jabwar japqua kalphe kauama laugul layalb lcspet leasan leater1 lessca lesyel lobdow lotjae \\\n","                      madpet magpet1 mallar3 masboo mauala maupar merlin mitpar moudov norcar norhar2 normoc norpin \\\n","                      norsho nutman oahama omao osprey pagplo palila parjae pecsan peflov perfal pibgre pomjae puaioh \\\n","                      reccar redava redjun redpha1 refboo rempar rettro ribgul rinduc rinphe rocpig rorpar rudtur ruff \\\n","                      saffin sander semplo sheowl shtsan skylar snogoo sooshe sooter1 sopsku1 sora spodov sposan \\\n","                      towsol wantat1 warwhe1 wesmea wessan wetshe whfibi whiter whttro wiltur yebcar yefcan zebdov'.split()\n","\n","    img_size = 224 #224 # 128\n","    main_metric = \"epoch_f1_at_03\"\n","\n","    period = 5\n","    n_mels = 224 #224 # 128\n","    fmin = 20\n","    fmax = 16000\n","    n_fft = 2048\n","    hop_length = 512\n","    sample_rate = 32000\n","    melspectrogram_parameters = {\n","        \"n_mels\": 224, #224, # 128,\n","        \"fmin\": 20,\n","        \"fmax\": 16000\n","    }\n","    \n","    \n","class AudioParams:\n","    \"\"\"\n","    Parameters used for the audio data\n","    \"\"\"\n","    sr = CFG.sample_rate\n","    duration = CFG.period\n","    # Melspectrogram\n","    n_mels = CFG.n_mels\n","    fmin = CFG.fmin\n","    fmax = CFG.fmax"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["class Compose:\n","    def __init__(self, transforms: list):\n","        self.transforms = transforms\n","\n","    def __call__(self, y: np.ndarray, sr):\n","        for trns in self.transforms:\n","            y = trns(y, sr)\n","        return y\n","\n","\n","class AudioTransform:\n","    def __init__(self, always_apply=False, p=0.5):\n","        self.always_apply = always_apply\n","        self.p = p\n","\n","    def __call__(self, y: np.ndarray, sr):\n","        if self.always_apply:\n","            return self.apply(y, sr=sr)\n","        else:\n","            if np.random.rand() < self.p:\n","                return self.apply(y, sr=sr)\n","            else:\n","                return y\n","\n","    def apply(self, y: np.ndarray, **params):\n","        raise NotImplementedError\n","\n","\n","class OneOf(Compose):\n","    # https://github.com/albumentations-team/albumentations/blob/master/albumentations/core/composition.py\n","    def __init__(self, transforms, p=0.5):\n","        super().__init__(transforms)\n","        self.p = p\n","        transforms_ps = [t.p for t in transforms]\n","        s = sum(transforms_ps)\n","        self.transforms_ps = [t / s for t in transforms_ps]\n","\n","    def __call__(self, y: np.ndarray, sr):\n","        data = y\n","        if self.transforms_ps and (random.random() < self.p):\n","            random_state = np.random.RandomState(random.randint(0, 2 ** 32 - 1))\n","            t = random_state.choice(self.transforms, p=self.transforms_ps)\n","            data = t(y, sr)\n","        return data\n","\n","\n","class Normalize(AudioTransform):\n","    def __init__(self, always_apply=False, p=1):\n","        super().__init__(always_apply, p)\n","\n","    def apply(self, y: np.ndarray, **params):\n","        max_vol = np.abs(y).max()\n","        y_vol = y * 1 / max_vol\n","        return np.asfortranarray(y_vol)\n","\n","\n","class NewNormalize(AudioTransform):\n","    def __init__(self, always_apply=False, p=1):\n","        super().__init__(always_apply, p)\n","\n","    def apply(self, y: np.ndarray, **params):\n","        y_mm = y - y.mean()\n","        return y_mm / y_mm.abs().max()\n","\n","\n","class NoiseInjection(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, max_noise_level=0.5):\n","        super().__init__(always_apply, p)\n","\n","        self.noise_level = (0.0, max_noise_level)\n","\n","    def apply(self, y: np.ndarray, **params):\n","        noise_level = np.random.uniform(*self.noise_level)\n","        noise = np.random.randn(len(y))\n","        augmented = (y + noise * noise_level).astype(y.dtype)\n","        return augmented\n","\n","\n","class GaussianNoise(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20):\n","        super().__init__(always_apply, p)\n","\n","        self.min_snr = min_snr\n","        self.max_snr = max_snr\n","\n","    def apply(self, y: np.ndarray, **params):\n","        snr = np.random.uniform(self.min_snr, self.max_snr)\n","        a_signal = np.sqrt(y ** 2).max()\n","        a_noise = a_signal / (10 ** (snr / 20))\n","\n","        white_noise = np.random.randn(len(y))\n","        a_white = np.sqrt(white_noise ** 2).max()\n","        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n","        return augmented\n","\n","#https://github.com/felixpatzelt/colorednoise\n","class PinkNoise(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20):\n","        super().__init__(always_apply, p)\n","\n","        self.min_snr = min_snr\n","        self.max_snr = max_snr\n","\n","    def apply(self, y: np.ndarray, **params):\n","        snr = np.random.uniform(self.min_snr, self.max_snr)\n","        a_signal = np.sqrt(y ** 2).max()\n","        a_noise = a_signal / (10 ** (snr / 20))\n","\n","        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n","        a_pink = np.sqrt(pink_noise ** 2).max()\n","        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n","        return augmented\n","\n","#https://github.com/felixpatzelt/colorednoise\n","class BrownNoise(AudioTransform):       #Added in V2\n","    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20):\n","        super().__init__(always_apply, p)\n","\n","        self.min_snr = min_snr\n","        self.max_snr = max_snr\n","\n","    def apply(self, y: np.ndarray, **params):\n","        snr = np.random.uniform(self.min_snr, self.max_snr)\n","        a_signal = np.sqrt(y ** 2).max()\n","        a_noise = a_signal / (10 ** (snr / 20))\n","\n","        brown_noise = cn.powerlaw_psd_gaussian(2, len(y))\n","        a_brown = np.sqrt(brown_noise ** 2).max()\n","        augmented = (y + brown_noise * 1 / a_brown * a_noise).astype(y.dtype)\n","        return augmented\n","\n","#https://www.kaggle.com/code/hidehisaarai1213/rfcx-audio-data-augmentation-japanese-english\n","#https://medium.com/@makcedward/data-augmentation-for-audio-76912b01fdf6\n","#Not implemented this yet, try in V3.\n","\n","class TimeShift(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, max_shift_second=2, sr=32000, padding_mode=\"replace\"):\n","        super().__init__(always_apply, p)\n","    \n","        assert padding_mode in [\"replace\", \"zero\"], \"`padding_mode` must be either 'replace' or 'zero'\"\n","        self.max_shift_second = max_shift_second\n","        self.sr = sr\n","        self.padding_mode = padding_mode\n","\n","    def apply(self, y: np.ndarray, **params):\n","        shift = np.random.randint(-self.sr * self.max_shift_second, self.sr * self.max_shift_second)\n","        augmented = np.roll(y, shift)\n","        if self.padding_mode == \"zero\":\n","            if shift > 0:\n","                augmented[:shift] = 0\n","            else:\n","                augmented[shift:] = 0\n","        return augmented\n","\n","\n","\n","class AddBackround_1(AudioTransform):       #Added in V3, using a single 30 second clip\n","    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20):\n","        super().__init__(always_apply, p)\n","\n","        self.min_snr = min_snr\n","        self.max_snr = max_snr\n","\n","    def apply(self, y: np.ndarray, **params):\n","        snr = np.random.uniform(self.min_snr, self.max_snr)\n","        background_dir = 'input/background_np_1/'\n","        filename = random.choice(os.listdir(background_dir))\n","        file_path = os.path.join(background_dir, filename)\n","\n","        a_signal = np.sqrt(y ** 2).max()\n","        a_noise = a_signal / (10 ** (snr / 20))  \n","        l_signal = len(y)\n","\n","        background = np.load(file_path)\n","        a_background = np.sqrt(background ** 2).max()\n","        l_background = len(background)\n","\n","        if l_signal > l_background:\n","            ratio = l_signal//l_background\n","            background = np.tile(background, ratio+1 )\n","            background = background[0:l_signal]\n","\n","        if l_signal < l_background:\n","            background = background[0:l_signal]\n","\n","        \n","        augmented = (y + background * 1 / a_background * a_noise).astype(y.dtype)\n","        return augmented\n","\n","#not used\n","class PitchShift(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, max_range=5):\n","        super().__init__(always_apply, p)\n","        self.max_range = max_range\n","\n","    def apply(self, y: np.ndarray, sr, **params):\n","        n_steps = np.random.randint(-self.max_range, self.max_range)\n","        augmented = librosa.effects.pitch_shift(y, sr, n_steps)\n","        return augmented\n","\n","#not used\n","class TimeStretch(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, max_rate=1):\n","        super().__init__(always_apply, p)\n","        self.max_rate = max_rate\n","\n","    def apply(self, y: np.ndarray, **params):\n","        rate = np.random.uniform(0, self.max_rate)\n","        augmented = librosa.effects.time_stretch(y, rate)\n","        return augmented\n","\n","\n","def _db2float(db: float, amplitude=True):\n","    if amplitude:\n","        return 10 ** (db / 20)\n","    else:\n","        return 10 ** (db / 10)\n","\n","\n","def volume_down(y: np.ndarray, db: float):\n","    \"\"\"\n","    Low level API for decreasing the volume\n","    Parameters\n","    ----------\n","    y: numpy.ndarray\n","        stereo / monaural input audio\n","    db: float\n","        how much decibel to decrease\n","    Returns\n","    -------\n","    applied: numpy.ndarray\n","        audio with decreased volume\n","    \"\"\"\n","    applied = y * _db2float(-db)\n","    return applied\n","\n","\n","def volume_up(y: np.ndarray, db: float):\n","    \"\"\"\n","    Low level API for increasing the volume\n","    Parameters\n","    ----------\n","    y: numpy.ndarray\n","        stereo / monaural input audio\n","    db: float\n","        how much decibel to increase\n","    Returns\n","    -------\n","    applied: numpy.ndarray\n","        audio with increased volume\n","    \"\"\"\n","    applied = y * _db2float(db)\n","    return applied\n","\n","\n","class RandomVolume(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, limit=10):\n","        super().__init__(always_apply, p)\n","        self.limit = limit\n","\n","    def apply(self, y: np.ndarray, **params):\n","        db = np.random.uniform(-self.limit, self.limit)\n","        if db >= 0:\n","            return volume_up(y, db)\n","        else:\n","            return volume_down(y, db)\n","\n","# not used\n","class CosineVolume(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, limit=10):\n","        super().__init__(always_apply, p)\n","        self.limit = limit\n","\n","    def apply(self, y: np.ndarray, **params):\n","        db = np.random.uniform(-self.limit, self.limit)\n","        cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n","        dbs = _db2float(cosine * db)\n","        return y * dbs"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["OUTPUT_DIR = f'output'    #was ./ for Kaggle\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)\n","   \n","    \n","def set_seed(seed=42):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    \n","set_seed(CFG.seed)"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["def calc_loss(y_true, y_pred):\n","    return metrics.roc_auc_score(np.array(y_true), np.array(y_pred))\n","\n","\n","# ====================================================\n","# Training helper functions\n","# ====================================================\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        \n","\n","class MetricMeter(object):\n","    def __init__(self):\n","        self.reset()\n","    \n","    def reset(self):\n","        self.y_true = []\n","        self.y_pred = []\n","    \n","    def update(self, y_true, y_pred):\n","        self.y_true.extend(y_true.cpu().detach().numpy().tolist())\n","        self.y_pred.extend(y_pred[\"clipwise_output\"].cpu().detach().numpy().tolist())\n","\n","    @property\n","    def avg(self):\n","        self.f1_03 = metrics.f1_score(np.array(self.y_true), np.array(self.y_pred) > 0.3, average=\"micro\")\n","        self.f1_05 = metrics.f1_score(np.array(self.y_true), np.array(self.y_pred) > 0.5, average=\"micro\")\n","        \n","        return {\n","            \"f1_at_03\" : self.f1_03,\n","            \"f1_at_05\" : self.f1_05,\n","        }\n","    \n","    \n","# https://www.kaggle.com/c/rfcx-species-audio-detection/discussion/213075\n","class BCEFocalLoss(nn.Module):\n","    def __init__(self, alpha=0.25, gamma=2.0):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, preds, targets):\n","        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(preds, targets)\n","        probas = torch.sigmoid(preds)\n","        loss = targets * self.alpha * \\\n","            (1. - probas)**self.gamma * bce_loss + \\\n","            (1. - targets) * probas**self.gamma * bce_loss\n","        loss = loss.mean()\n","        return loss\n","\n","\n","class BCEFocal2WayLoss(nn.Module):\n","    def __init__(self, weights=[1, 1], class_weights=None):\n","        super().__init__()\n","\n","        self.focal = BCEFocalLoss()\n","\n","        self.weights = weights\n","\n","    def forward(self, input, target):\n","        input_ = input[\"logit\"]\n","        target = target.float()\n","\n","        framewise_output = input[\"framewise_logit\"]\n","        clipwise_output_with_max, _ = framewise_output.max(dim=1)\n","\n","        loss = self.focal(input_, target)\n","        aux_loss = self.focal(clipwise_output_with_max, target)\n","\n","        return self.weights[0] * loss + self.weights[1] * aux_loss"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/albumentations/augmentations/transforms.py:689: FutureWarning: This class has been deprecated. Please use CoarseDropout\n","  warnings.warn(\n"]}],"source":["def compute_melspec(y, params):\n","    \"\"\"\n","    Computes a mel-spectrogram and puts it at decibel scale\n","    Arguments:\n","        y {np array} -- signal\n","        params {AudioParams} -- Parameters to use for the spectrogram. Expected to have the attributes sr, n_mels, f_min, f_max\n","    Returns:\n","        np array -- Mel-spectrogram\n","    \"\"\"\n","    melspec = librosa.feature.melspectrogram(\n","        y=y, sr=params.sr, n_mels=params.n_mels, fmin=params.fmin, fmax=params.fmax,\n","    )\n","\n","    melspec = librosa.power_to_db(melspec).astype(np.float32)\n","    return melspec\n","\n","\n","def crop_or_pad(y, length, sr, train=True, probs=None):\n","    \"\"\"\n","    Crops an array to a chosen length\n","    Arguments:\n","        y {1D np array} -- Array to crop\n","        length {int} -- Length of the crop\n","        sr {int} -- Sampling rate\n","    Keyword Arguments:\n","        train {bool} -- Whether we are at train time. If so, crop randomly, else return the beginning of y (default: {True})\n","        probs {None or numpy array} -- Probabilities to use to chose where to crop (default: {None})\n","    Returns:\n","        1D np array -- Cropped array\n","    \"\"\"\n","    if len(y) <= length:\n","        y = np.concatenate([y, np.zeros(length - len(y))])\n","    else:\n","        if not train:\n","            start = 0\n","        elif probs is None:\n","            start = np.random.randint(len(y) - length)\n","        else:\n","            start = (\n","                    np.random.choice(np.arange(len(probs)), p=probs) + np.random.random()\n","            )\n","            start = int(sr * (start))\n","\n","        y = y[start: start + length]\n","\n","    return y.astype(np.float32)\n","\n","\n","def mono_to_color(X, eps=1e-6, mean=None, std=None):\n","    \"\"\"\n","    Converts a one channel array to a 3 channel one in [0, 255]\n","    Arguments:\n","        X {numpy array [H x W]} -- 2D array to convert\n","    Keyword Arguments:\n","        eps {float} -- To avoid dividing by 0 (default: {1e-6})\n","        mean {None or np array} -- Mean for normalization (default: {None})\n","        std {None or np array} -- Std for normalization (default: {None})\n","    Returns:\n","        numpy array [3 x H x W] -- RGB numpy array\n","    \"\"\"\n","    X = np.stack([X, X, X], axis=-1)\n","\n","    # Standardize\n","    mean = mean or X.mean()\n","    std = std or X.std()\n","    X = (X - mean) / (std + eps)\n","\n","    # Normalize to [0, 255]\n","    _min, _max = X.min(), X.max()\n","\n","    if (_max - _min) > eps:\n","        V = np.clip(X, _min, _max)\n","        V = 255 * (V - _min) / (_max - _min)\n","        V = V.astype(np.uint8)\n","    else:\n","        V = np.zeros_like(X, dtype=np.uint8)\n","\n","    return V\n","\n","\n","mean = (0.485, 0.456, 0.406) # RGB\n","std = (0.229, 0.224, 0.225) # RGB\n","\n","albu_transforms = {\n","    'train' : A.Compose([\n","            A.HorizontalFlip(p=0.5),\n","            A.OneOf([\n","                A.Cutout(max_h_size=5, max_w_size=16),\n","                A.CoarseDropout(max_holes=4),\n","            ], p=0.5),\n","            A.Normalize(mean, std),\n","    ]),\n","    'valid' : A.Compose([\n","            A.Normalize(mean, std),\n","    ]),\n","}\n","\n","\n","\n","# This was the example given\n","#transform = TimeShift(always_apply=True, max_shift_second=4, sr=sr)\n","#y_time_shifted = transform(y)\n","#Audio(y_time_shifted, rate=sr)\n","\n","class WaveformDataset(torch.utils.data.Dataset):\n","    def __init__(self,\n","                 df: pd.DataFrame,\n","                 mode='train'):\n","        self.df = df\n","        self.mode = mode\n","\n","        if mode == 'train':\n","            self.wave_transforms = Compose(\n","                [\n","                    OneOf(\n","                        [\n","                            NoiseInjection(p=1, max_noise_level=0.04),\n","                            GaussianNoise(p=1, min_snr=5, max_snr=20),\n","                            PinkNoise(p=1, min_snr=5, max_snr=20),\n","                            BrownNoise(p=1, min_snr=5, max_snr=20),     #Added in V2\n","                        ],\n","                        p=0.2,\n","                    ),\n","                    AddBackround_1(p=0.4, min_snr=5, max_snr=20),\n","                    TimeShift(p=0.2, max_shift_second=0.5),  #I'm worried it's too much given they are only 5 second clips\n","                    RandomVolume(p=0.2, limit=4),\n","                    Normalize(p=1),\n","                ]\n","            )\n","        else:\n","            self.wave_transforms = Compose(\n","                [\n","                    Normalize(p=1),\n","                ]\n","            )\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx: int):\n","        SR = 32000\n","        sample = self.df.loc[idx, :]\n","        \n","        wav_path = sample[\"file_path\"]\n","        labels = sample[\"new_target\"]\n","\n","        y = np.load(wav_path)\n","\n","        # SEC = int(len(y)/2/SR)\n","        # if SEC > 0:\n","        #     start = np.random.randint(SEC)\n","        #     end = start+AudioParams.duration\n","        if len(y) > 0:\n","            y = y[:AudioParams.duration*SR]\n","\n","            if self.wave_transforms:\n","                y = self.wave_transforms(y, sr=SR)\n","\n","        y = np.concatenate([y, y, y])[:AudioParams.duration * AudioParams.sr] \n","        y = crop_or_pad(y, AudioParams.duration * AudioParams.sr, sr=AudioParams.sr, train=True, probs=None)\n","        image = compute_melspec(y, AudioParams)\n","        image = mono_to_color(image)\n","        image = image.astype(np.uint8)\n","        \n","        # image = np.load(wav_path) # (224, 313, 3)\n","        image = albu_transforms[self.mode](image=image)['image']\n","        image = image.T\n","        \n","        targets = np.zeros(len(CFG.target_columns), dtype=float)\n","        for ebird_code in labels.split():\n","            targets[CFG.target_columns.index(ebird_code)] = 1.0\n","\n","        return {\n","            \"image\": image,\n","            \"targets\": targets,\n","        }"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["def init_layer(layer):\n","    nn.init.xavier_uniform_(layer.weight)\n","\n","    if hasattr(layer, \"bias\"):\n","        if layer.bias is not None:\n","            layer.bias.data.fill_(0.)\n","\n","\n","def init_bn(bn):\n","    bn.bias.data.fill_(0.)\n","    bn.weight.data.fill_(1.0)\n","\n","\n","def init_weights(model):\n","    classname = model.__class__.__name__\n","    if classname.find(\"Conv2d\") != -1:\n","        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n","        model.bias.data.fill_(0)\n","    elif classname.find(\"BatchNorm\") != -1:\n","        model.weight.data.normal_(1.0, 0.02)\n","        model.bias.data.fill_(0)\n","    elif classname.find(\"GRU\") != -1:\n","        for weight in model.parameters():\n","            if len(weight.size()) > 1:\n","                nn.init.orghogonal_(weight.data)\n","    elif classname.find(\"Linear\") != -1:\n","        model.weight.data.normal_(0, 0.01)\n","        model.bias.data.zero_()\n","\n","\n","def interpolate(x: torch.Tensor, ratio: int):\n","    \"\"\"Interpolate data in time domain. This is used to compensate the\n","    resolution reduction in downsampling of a CNN.\n","    Args:\n","      x: (batch_size, time_steps, classes_num)\n","      ratio: int, ratio to interpolate\n","    Returns:\n","      upsampled: (batch_size, time_steps * ratio, classes_num)\n","    \"\"\"\n","    (batch_size, time_steps, classes_num) = x.shape\n","    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n","    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n","    return upsampled\n","\n","\n","def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n","    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n","    is the same as the value of the last frame.\n","    Args:\n","      framewise_output: (batch_size, frames_num, classes_num)\n","      frames_num: int, number of frames to pad\n","    Outputs:\n","      output: (batch_size, frames_num, classes_num)\n","    \"\"\"\n","    output = F.interpolate(\n","        framewise_output.unsqueeze(1),\n","        size=(frames_num, framewise_output.size(2)),\n","        align_corners=True,\n","        mode=\"bilinear\").squeeze(1)\n","\n","    return output\n","\n","\n","class AttBlockV2(nn.Module):\n","    def __init__(self,\n","                 in_features: int,\n","                 out_features: int,\n","                 activation=\"linear\"):\n","        super().__init__()\n","\n","        self.activation = activation\n","        self.att = nn.Conv1d(\n","            in_channels=in_features,\n","            out_channels=out_features,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=True)\n","        self.cla = nn.Conv1d(\n","            in_channels=in_features,\n","            out_channels=out_features,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=True)\n","\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        init_layer(self.att)\n","        init_layer(self.cla)\n","\n","    def forward(self, x):\n","        # x: (n_samples, n_in, n_time)\n","        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n","        cla = self.nonlinear_transform(self.cla(x))\n","        x = torch.sum(norm_att * cla, dim=2)\n","        return x, norm_att, cla\n","\n","    def nonlinear_transform(self, x):\n","        if self.activation == 'linear':\n","            return x\n","        elif self.activation == 'sigmoid':\n","            return torch.sigmoid(x)\n","\n","\n","class TimmSED(nn.Module):\n","    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1):\n","        super().__init__()\n","\n","        self.spec_augmenter = SpecAugmentation(time_drop_width=64//2, time_stripes_num=2,\n","                                               freq_drop_width=8//2, freq_stripes_num=2)\n","\n","        self.bn0 = nn.BatchNorm2d(CFG.n_mels)\n","\n","        base_model = timm.create_model(\n","            base_model_name, pretrained=pretrained, in_chans=in_channels)\n","        layers = list(base_model.children())[:-2]\n","        self.encoder = nn.Sequential(*layers)\n","\n","        if hasattr(base_model, \"fc\"):\n","            in_features = base_model.fc.in_features\n","        else:\n","            in_features = base_model.classifier.in_features\n","\n","        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n","        self.att_block = AttBlockV2(\n","            in_features, num_classes, activation=\"sigmoid\")\n","\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        init_bn(self.bn0)\n","        init_layer(self.fc1)\n","        \n","\n","    def forward(self, input_data):\n","        x = input_data # (batch_size, 3, time_steps, mel_bins)\n","\n","        frames_num = x.shape[2]\n","\n","        x = x.transpose(1, 3)\n","        x = self.bn0(x)\n","        x = x.transpose(1, 3)\n","\n","        if self.training:\n","            if random.random() < 0.25:\n","                x = self.spec_augmenter(x)\n","\n","        x = x.transpose(2, 3)\n","\n","        x = self.encoder(x)\n","        \n","        # Aggregate in frequency axis\n","        x = torch.mean(x, dim=3)\n","\n","        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n","        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n","        x = x1 + x2\n","\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = x.transpose(1, 2)\n","        x = F.relu_(self.fc1(x))\n","        x = x.transpose(1, 2)\n","        x = F.dropout(x, p=0.5, training=self.training)\n","\n","        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n","        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n","        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n","        segmentwise_output = segmentwise_output.transpose(1, 2)\n","\n","        interpolate_ratio = frames_num // segmentwise_output.size(1)\n","\n","        # Get framewise output\n","        framewise_output = interpolate(segmentwise_output,\n","                                       interpolate_ratio)\n","        framewise_output = pad_framewise_output(framewise_output, frames_num)\n","\n","        framewise_logit = interpolate(segmentwise_logit, interpolate_ratio)\n","        framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n","\n","        output_dict = {\n","            'framewise_output': framewise_output,\n","            'clipwise_output': clipwise_output,\n","            'logit': logit,\n","            'framewise_logit': framewise_logit,\n","        }\n","\n","        return output_dict"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["def rand_bbox(size, lam):\n","    W = size[2]\n","    H = size[3]\n","    cut_rat = np.sqrt(1. - lam)\n","    cut_w = int(W * cut_rat)\n","    cut_h = int(H * cut_rat)\n","\n","    # uniform\n","    cx = np.random.randint(W)\n","    cy = np.random.randint(H)\n","\n","    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n","    bby1 = np.clip(cy - cut_h // 2, 0, H)\n","    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n","    bby2 = np.clip(cy + cut_h // 2, 0, H)\n","\n","    return bbx1, bby1, bbx2, bby2\n","\n","\n","def cutmix(data, targets, alpha):\n","    indices = torch.randperm(data.size(0))\n","    shuffled_data = data[indices]\n","    shuffled_targets = targets[indices]\n","\n","    lam = np.random.beta(alpha, alpha)\n","    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n","    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n","    # adjust lambda to exactly match pixel ratio\n","    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n","\n","    new_targets = [targets, shuffled_targets, lam]\n","    return data, new_targets\n","\n","def mixup(data, targets, alpha):\n","    indices = torch.randperm(data.size(0))\n","    shuffled_data = data[indices]\n","    shuffled_targets = targets[indices]\n","\n","    lam = np.random.beta(alpha, alpha)\n","    new_data = data * lam + shuffled_data * (1 - lam)\n","    new_targets = [targets, shuffled_targets, lam]\n","    return new_data, new_targets\n","\n","\n","def cutmix_criterion(preds, new_targets):\n","    targets1, targets2, lam = new_targets[0], new_targets[1], new_targets[2]\n","    criterion = BCEFocal2WayLoss()\n","    return lam * criterion(preds, targets1) + (1 - lam) * criterion(preds, targets2)\n","\n","def mixup_criterion(preds, new_targets):\n","    targets1, targets2, lam = new_targets[0], new_targets[1], new_targets[2]\n","    criterion = BCEFocal2WayLoss()\n","    return lam * criterion(preds, targets1) + (1 - lam) * criterion(preds, targets2)\n","\n","\n","def loss_fn(logits, targets):\n","    loss_fct = BCEFocal2WayLoss()\n","    loss = loss_fct(logits, targets)\n","    return loss"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["def train_fn(model, data_loader, device, optimizer, scheduler):\n","    model.train()\n","    scaler = GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    scores = MetricMeter()\n","    tk0 = tqdm(data_loader, total=len(data_loader))\n","    \n","    for data in tk0:\n","        optimizer.zero_grad()\n","        inputs = data['image'].to(device)\n","        targets = data['targets'].to(device)\n","        with autocast(enabled=CFG.apex):\n","            outputs = model(inputs)\n","            loss = loss_fn(outputs, targets)\n","        \n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        \n","        scheduler.step()\n","        losses.update(loss.item(), inputs.size(0))\n","        scores.update(targets, outputs)\n","        tk0.set_postfix(loss=losses.avg)\n","    return scores.avg, losses.avg\n","\n","\n","def train_mixup_cutmix_fn(model, data_loader, device, optimizer, scheduler):\n","    model.train()\n","    scaler = GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    scores = MetricMeter()\n","    tk0 = tqdm(data_loader, total=len(data_loader))\n","\n","    for data in tk0:\n","        optimizer.zero_grad()\n","        inputs = data['image'].to(device)\n","        targets = data['targets'].to(device)\n","\n","        if np.random.rand()<0.5:\n","            inputs, new_targets = mixup(inputs, targets, 0.4)\n","            with autocast(enabled=CFG.apex):\n","                outputs = model(inputs)\n","                loss = mixup_criterion(outputs, new_targets) \n","        else:\n","            inputs, new_targets = cutmix(inputs, targets, 0.4)\n","            with autocast(enabled=CFG.apex):\n","                outputs = model(inputs)\n","                loss = cutmix_criterion(outputs, new_targets)\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        \n","        scheduler.step()\n","        losses.update(loss.item(), inputs.size(0))\n","        scores.update(new_targets[0], outputs)\n","        tk0.set_postfix(loss=losses.avg)\n","    return scores.avg, losses.avg\n","\n","\n","def valid_fn(model, data_loader, device):\n","    model.eval()\n","    losses = AverageMeter()\n","    scores = MetricMeter()\n","    tk0 = tqdm(data_loader, total=len(data_loader))\n","    valid_preds = []\n","    with torch.no_grad():\n","        for data in tk0:\n","            inputs = data['image'].to(device)\n","            targets = data['targets'].to(device)\n","            outputs = model(inputs)\n","            loss = loss_fn(outputs, targets)\n","            losses.update(loss.item(), inputs.size(0))\n","            scores.update(targets, outputs)\n","            tk0.set_postfix(loss=losses.avg)\n","    return scores.avg, losses.avg"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[],"source":["def inference_fn(model, data_loader, device):\n","    model.eval()\n","    tk0 = tqdm(data_loader, total=len(data_loader))\n","    final_output = []\n","    final_target = []\n","    with torch.no_grad():\n","        for b_idx, data in enumerate(tk0):\n","            inputs = data['image'].to(device)\n","            targets = data['targets'].to(device).detach().cpu().numpy().tolist()\n","            output = model(inputs)\n","            output = output[\"clipwise_output\"].cpu().detach().cpu().numpy().tolist()\n","            final_output.extend(output)\n","            final_target.extend(targets)\n","    return final_output, final_target\n","\n","\n","def calc_cv(model_paths):\n","    df = pd.read_csv('train_folds.csv')\n","    y_true = []\n","    y_pred = []\n","    for fold, model_path in enumerate(model_paths):\n","        model = TimmSED(\n","            base_model_name=CFG.base_model_name,\n","            pretrained=CFG.pretrained,\n","            num_classes=CFG.num_classes,\n","            in_channels=CFG.in_channels)\n","\n","        model.to(device)\n","        model.load_state_dict(torch.load(model_path))\n","        model.eval()\n","\n","        val_df = df[df.kfold == fold].reset_index(drop=True)\n","        dataset = WaveformDataset(df=val_df, mode='valid')\n","        dataloader = torch.utils.data.DataLoader(\n","            dataset, batch_size=CFG.valid_bs, num_workers=0, pin_memory=True, shuffle=False\n","        )\n","\n","        final_output, final_target = inference_fn(model, dataloader, device)\n","        y_pred.extend(final_output)\n","        y_true.extend(final_target)\n","        torch.cuda.empty_cache()\n","\n","        f1_03 = metrics.f1_score(np.array(y_true), np.array(y_pred) > 0.3, average=\"micro\")\n","        print(f'micro f1_0.3 {f1_03}')\n","\n","    f1_03 = metrics.f1_score(np.array(y_true), np.array(y_pred) > 0.3, average=\"micro\")\n","    f1_05 = metrics.f1_score(np.array(y_true), np.array(y_pred) > 0.5, average=\"micro\")\n","\n","    print(f'overall micro f1_0.3 {f1_03}')\n","    print(f'overall micro f1_0.5 {f1_05}')\n","    return"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["====================================================================================================\n","Fold 4 Training\n","====================================================================================================\n"]},{"name":"stderr","output_type":"stream","text":["/home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Starting 1 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [17:22<00:00,  1.40s/it, loss=0.0143]\n","100%|| 93/93 [01:09<00:00,  1.34it/s, loss=0.00791]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.01428  avg_val_loss: 0.00791  time: 1113s\n","Epoch 1 - train_f1_at_03:0.00458  valid_f1_at_03:0.00180\n","Epoch 1 - train_f1_at_05:0.00175  valid_f1_at_05:0.00000\n",">>>>>>>> Model Improved From -inf ----> 0.0017974835230677051\n","other scores here... 0.0017974835230677051, 0.0\n","Starting 2 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:53<00:00,  1.36s/it, loss=0.00843]\n","100%|| 93/93 [01:08<00:00,  1.36it/s, loss=0.00678]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.00843  avg_val_loss: 0.00678  time: 1082s\n","Epoch 2 - train_f1_at_03:0.01334  valid_f1_at_03:0.14465\n","Epoch 2 - train_f1_at_05:0.00060  valid_f1_at_05:0.01902\n",">>>>>>>> Model Improved From 0.0017974835230677051 ----> 0.14464905257539368\n","other scores here... 0.14464905257539368, 0.01902497027348395\n","Starting 3 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:57<00:00,  1.37s/it, loss=0.00778]\n","100%|| 93/93 [01:09<00:00,  1.34it/s, loss=0.00605]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.00778  avg_val_loss: 0.00605  time: 1087s\n","Epoch 3 - train_f1_at_03:0.04238  valid_f1_at_03:0.30005\n","Epoch 3 - train_f1_at_05:0.00210  valid_f1_at_05:0.10398\n",">>>>>>>> Model Improved From 0.14464905257539368 ----> 0.30004688232536336\n","other scores here... 0.30004688232536336, 0.10397727272727274\n","Starting 4 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:56<00:00,  1.37s/it, loss=0.00706]\n","100%|| 93/93 [01:09<00:00,  1.34it/s, loss=0.00593]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.00706  avg_val_loss: 0.00593  time: 1087s\n","Epoch 4 - train_f1_at_03:0.11600  valid_f1_at_03:0.29607\n","Epoch 4 - train_f1_at_05:0.01235  valid_f1_at_05:0.13333\n","Starting 5 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:58<00:00,  1.37s/it, loss=0.00677]\n","100%|| 93/93 [01:09<00:00,  1.34it/s, loss=0.00511]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.00677  avg_val_loss: 0.00511  time: 1088s\n","Epoch 5 - train_f1_at_03:0.15932  valid_f1_at_03:0.46175\n","Epoch 5 - train_f1_at_05:0.02116  valid_f1_at_05:0.25950\n",">>>>>>>> Model Improved From 0.30004688232536336 ----> 0.4617481020050613\n","other scores here... 0.4617481020050613, 0.2594985784440424\n","Starting 6 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:55<00:00,  1.37s/it, loss=0.00666]\n","100%|| 93/93 [01:08<00:00,  1.36it/s, loss=0.00464]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6 - avg_train_loss: 0.00666  avg_val_loss: 0.00464  time: 1085s\n","Epoch 6 - train_f1_at_03:0.18266  valid_f1_at_03:0.53151\n","Epoch 6 - train_f1_at_05:0.02510  valid_f1_at_05:0.29301\n",">>>>>>>> Model Improved From 0.4617481020050613 ----> 0.5315142198308993\n","other scores here... 0.5315142198308993, 0.2930066360387953\n","Starting 7 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:57<00:00,  1.37s/it, loss=0.00637]\n","100%|| 93/93 [01:09<00:00,  1.34it/s, loss=0.00488]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7 - avg_train_loss: 0.00637  avg_val_loss: 0.00488  time: 1087s\n","Epoch 7 - train_f1_at_03:0.20472  valid_f1_at_03:0.47912\n","Epoch 7 - train_f1_at_05:0.03121  valid_f1_at_05:0.25404\n","Starting 8 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:57<00:00,  1.37s/it, loss=0.00603]\n","100%|| 93/93 [01:09<00:00,  1.33it/s, loss=0.0049] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8 - avg_train_loss: 0.00603  avg_val_loss: 0.00490  time: 1088s\n","Epoch 8 - train_f1_at_03:0.24735  valid_f1_at_03:0.48628\n","Epoch 8 - train_f1_at_05:0.04766  valid_f1_at_05:0.28564\n","Starting 9 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:57<00:00,  1.37s/it, loss=0.00602]\n","100%|| 93/93 [01:08<00:00,  1.36it/s, loss=0.00413]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9 - avg_train_loss: 0.00602  avg_val_loss: 0.00413  time: 1087s\n","Epoch 9 - train_f1_at_03:0.25886  valid_f1_at_03:0.59315\n","Epoch 9 - train_f1_at_05:0.04416  valid_f1_at_05:0.42536\n",">>>>>>>> Model Improved From 0.5315142198308993 ----> 0.5931531531531532\n","other scores here... 0.5931531531531532, 0.4253626579316799\n","Starting 10 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [17:00<00:00,  1.37s/it, loss=0.00595]\n","100%|| 93/93 [01:09<00:00,  1.34it/s, loss=0.00393]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10 - avg_train_loss: 0.00595  avg_val_loss: 0.00393  time: 1090s\n","Epoch 10 - train_f1_at_03:0.28204  valid_f1_at_03:0.61306\n","Epoch 10 - train_f1_at_05:0.05669  valid_f1_at_05:0.39886\n",">>>>>>>> Model Improved From 0.5931531531531532 ----> 0.6130634774609015\n","other scores here... 0.6130634774609015, 0.3988563259471051\n","Starting 11 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:55<00:00,  1.37s/it, loss=0.00567]\n","100%|| 93/93 [01:09<00:00,  1.33it/s, loss=0.00428]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 11 - avg_train_loss: 0.00567  avg_val_loss: 0.00428  time: 1086s\n","Epoch 11 - train_f1_at_03:0.29072  valid_f1_at_03:0.57105\n","Epoch 11 - train_f1_at_05:0.06669  valid_f1_at_05:0.33845\n","Starting 12 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:56<00:00,  1.37s/it, loss=0.00545]\n","100%|| 93/93 [01:10<00:00,  1.32it/s, loss=0.00459]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 12 - avg_train_loss: 0.00545  avg_val_loss: 0.00459  time: 1088s\n","Epoch 12 - train_f1_at_03:0.31986  valid_f1_at_03:0.55572\n","Epoch 12 - train_f1_at_05:0.07800  valid_f1_at_05:0.22575\n","Starting 13 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:55<00:00,  1.37s/it, loss=0.00557]\n","100%|| 93/93 [01:10<00:00,  1.32it/s, loss=0.00365]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 13 - avg_train_loss: 0.00557  avg_val_loss: 0.00365  time: 1087s\n","Epoch 13 - train_f1_at_03:0.32351  valid_f1_at_03:0.65150\n","Epoch 13 - train_f1_at_05:0.07046  valid_f1_at_05:0.47080\n",">>>>>>>> Model Improved From 0.6130634774609015 ----> 0.6514965867320146\n","other scores here... 0.6514965867320146, 0.47080209043399224\n","Starting 14 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:55<00:00,  1.37s/it, loss=0.00549]\n","100%|| 93/93 [01:10<00:00,  1.32it/s, loss=0.00357]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 14 - avg_train_loss: 0.00549  avg_val_loss: 0.00357  time: 1086s\n","Epoch 14 - train_f1_at_03:0.32661  valid_f1_at_03:0.66073\n","Epoch 14 - train_f1_at_05:0.08434  valid_f1_at_05:0.47725\n",">>>>>>>> Model Improved From 0.6514965867320146 ----> 0.6607301869991095\n","other scores here... 0.6607301869991095, 0.47724700022639793\n","Starting 15 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:56<00:00,  1.37s/it, loss=0.00536]\n","100%|| 93/93 [01:08<00:00,  1.36it/s, loss=0.00396]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 15 - avg_train_loss: 0.00536  avg_val_loss: 0.00396  time: 1085s\n","Epoch 15 - train_f1_at_03:0.34408  valid_f1_at_03:0.62045\n","Epoch 15 - train_f1_at_05:0.09408  valid_f1_at_05:0.34353\n","Starting 16 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:53<00:00,  1.36s/it, loss=0.00513]\n","100%|| 93/93 [01:09<00:00,  1.34it/s, loss=0.00384]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 16 - avg_train_loss: 0.00513  avg_val_loss: 0.00384  time: 1083s\n","Epoch 16 - train_f1_at_03:0.35139  valid_f1_at_03:0.63636\n","Epoch 16 - train_f1_at_05:0.09992  valid_f1_at_05:0.46612\n","Starting 17 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:54<00:00,  1.37s/it, loss=0.00516]\n","100%|| 93/93 [01:09<00:00,  1.34it/s, loss=0.00337]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 17 - avg_train_loss: 0.00516  avg_val_loss: 0.00337  time: 1085s\n","Epoch 17 - train_f1_at_03:0.36854  valid_f1_at_03:0.68414\n","Epoch 17 - train_f1_at_05:0.12217  valid_f1_at_05:0.51979\n",">>>>>>>> Model Improved From 0.6607301869991095 ----> 0.6841379310344827\n","other scores here... 0.6841379310344827, 0.5197889182058048\n","Starting 18 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:53<00:00,  1.36s/it, loss=0.00514]\n","100%|| 93/93 [01:09<00:00,  1.34it/s, loss=0.00341]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 18 - avg_train_loss: 0.00514  avg_val_loss: 0.00341  time: 1084s\n","Epoch 18 - train_f1_at_03:0.36940  valid_f1_at_03:0.68429\n","Epoch 18 - train_f1_at_05:0.11695  valid_f1_at_05:0.47574\n",">>>>>>>> Model Improved From 0.6841379310344827 ----> 0.6842934687445281\n","other scores here... 0.6842934687445281, 0.4757369614512472\n","Starting 19 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:53<00:00,  1.36s/it, loss=0.00345]\n","100%|| 93/93 [01:08<00:00,  1.36it/s, loss=0.00432]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 19 - avg_train_loss: 0.00345  avg_val_loss: 0.00432  time: 1083s\n","Epoch 19 - train_f1_at_03:0.67248  valid_f1_at_03:0.58637\n","Epoch 19 - train_f1_at_05:0.46655  valid_f1_at_05:0.46858\n","Starting 20 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:54<00:00,  1.37s/it, loss=0.00327]\n","100%|| 93/93 [01:10<00:00,  1.33it/s, loss=0.00364]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 20 - avg_train_loss: 0.00327  avg_val_loss: 0.00364  time: 1086s\n","Epoch 20 - train_f1_at_03:0.69298  valid_f1_at_03:0.65507\n","Epoch 20 - train_f1_at_05:0.53508  valid_f1_at_05:0.55871\n","Starting 21 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:51<00:00,  1.36s/it, loss=0.00334]\n","100%|| 93/93 [01:08<00:00,  1.36it/s, loss=0.00349]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 21 - avg_train_loss: 0.00334  avg_val_loss: 0.00349  time: 1081s\n","Epoch 21 - train_f1_at_03:0.68805  valid_f1_at_03:0.70217\n","Epoch 21 - train_f1_at_05:0.53341  valid_f1_at_05:0.65182\n",">>>>>>>> Model Improved From 0.6842934687445281 ----> 0.7021663634860261\n","other scores here... 0.7021663634860261, 0.6518171160609613\n","Starting 22 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:53<00:00,  1.36s/it, loss=0.00328]\n","100%|| 93/93 [01:10<00:00,  1.33it/s, loss=0.00347]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 22 - avg_train_loss: 0.00328  avg_val_loss: 0.00347  time: 1084s\n","Epoch 22 - train_f1_at_03:0.70009  valid_f1_at_03:0.69664\n","Epoch 22 - train_f1_at_05:0.54925  valid_f1_at_05:0.63672\n","Starting 23 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:49<00:00,  1.36s/it, loss=0.00287]\n","100%|| 93/93 [01:09<00:00,  1.34it/s, loss=0.00417]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 23 - avg_train_loss: 0.00287  avg_val_loss: 0.00417  time: 1080s\n","Epoch 23 - train_f1_at_03:0.73885  valid_f1_at_03:0.64191\n","Epoch 23 - train_f1_at_05:0.60431  valid_f1_at_05:0.57025\n","Starting 24 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:50<00:00,  1.36s/it, loss=0.00281]\n","100%|| 93/93 [01:08<00:00,  1.35it/s, loss=0.00378]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 24 - avg_train_loss: 0.00281  avg_val_loss: 0.00378  time: 1080s\n","Epoch 24 - train_f1_at_03:0.74774  valid_f1_at_03:0.68761\n","Epoch 24 - train_f1_at_05:0.61140  valid_f1_at_05:0.63629\n","Starting 25 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:53<00:00,  1.36s/it, loss=0.00297]\n","100%|| 93/93 [01:08<00:00,  1.37it/s, loss=0.0033] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 25 - avg_train_loss: 0.00297  avg_val_loss: 0.00330  time: 1082s\n","Epoch 25 - train_f1_at_03:0.73119  valid_f1_at_03:0.71551\n","Epoch 25 - train_f1_at_05:0.59156  valid_f1_at_05:0.67462\n",">>>>>>>> Model Improved From 0.7021663634860261 ----> 0.7155059132720106\n","other scores here... 0.7155059132720106, 0.6746153846153846\n","Starting 26 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:52<00:00,  1.36s/it, loss=0.00286]\n","100%|| 93/93 [01:08<00:00,  1.36it/s, loss=0.00366]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 26 - avg_train_loss: 0.00286  avg_val_loss: 0.00366  time: 1081s\n","Epoch 26 - train_f1_at_03:0.74224  valid_f1_at_03:0.69562\n","Epoch 26 - train_f1_at_05:0.61343  valid_f1_at_05:0.66318\n","Starting 27 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:49<00:00,  1.36s/it, loss=0.0025] \n","100%|| 93/93 [01:07<00:00,  1.37it/s, loss=0.00416]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 27 - avg_train_loss: 0.00250  avg_val_loss: 0.00416  time: 1078s\n","Epoch 27 - train_f1_at_03:0.77664  valid_f1_at_03:0.65015\n","Epoch 27 - train_f1_at_05:0.66313  valid_f1_at_05:0.59610\n","Starting 28 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:49<00:00,  1.36s/it, loss=0.00247]\n","100%|| 93/93 [01:08<00:00,  1.36it/s, loss=0.00375]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 28 - avg_train_loss: 0.00247  avg_val_loss: 0.00375  time: 1079s\n","Epoch 28 - train_f1_at_03:0.78078  valid_f1_at_03:0.72039\n","Epoch 28 - train_f1_at_05:0.66687  valid_f1_at_05:0.69157\n",">>>>>>>> Model Improved From 0.7155059132720106 ----> 0.7203915171288743\n","other scores here... 0.7203915171288743, 0.6915676287492927\n","Starting 29 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:05<00:00,  1.30s/it, loss=0.00271]\n","100%|| 93/93 [01:01<00:00,  1.52it/s, loss=0.00356]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 29 - avg_train_loss: 0.00271  avg_val_loss: 0.00356  time: 1028s\n","Epoch 29 - train_f1_at_03:0.75444  valid_f1_at_03:0.71580\n","Epoch 29 - train_f1_at_05:0.63611  valid_f1_at_05:0.68700\n","Starting 30 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:14<00:00,  1.23s/it, loss=0.00249]\n","100%|| 93/93 [01:01<00:00,  1.52it/s, loss=0.00385]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 30 - avg_train_loss: 0.00249  avg_val_loss: 0.00385  time: 977s\n","Epoch 30 - train_f1_at_03:0.78025  valid_f1_at_03:0.69706\n","Epoch 30 - train_f1_at_05:0.66766  valid_f1_at_05:0.66347\n","Starting 31 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:14<00:00,  1.23s/it, loss=0.00227]\n","100%|| 93/93 [01:00<00:00,  1.54it/s, loss=0.00403]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 31 - avg_train_loss: 0.00227  avg_val_loss: 0.00403  time: 975s\n","Epoch 31 - train_f1_at_03:0.80228  valid_f1_at_03:0.66839\n","Epoch 31 - train_f1_at_05:0.70459  valid_f1_at_05:0.62189\n","Starting 32 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:12<00:00,  1.23s/it, loss=0.0023] \n","100%|| 93/93 [01:00<00:00,  1.53it/s, loss=0.00355]\n","/home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 32 - avg_train_loss: 0.00230  avg_val_loss: 0.00355  time: 974s\n","Epoch 32 - train_f1_at_03:0.80002  valid_f1_at_03:0.71092\n","Epoch 32 - train_f1_at_05:0.69893  valid_f1_at_05:0.68100\n","====================================================================================================\n","Fold 3 Training\n","====================================================================================================\n","Starting 1 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:15<00:00,  1.23s/it, loss=0.0145]\n","100%|| 93/93 [01:01<00:00,  1.51it/s, loss=0.00793]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.01447  avg_val_loss: 0.00793  time: 978s\n","Epoch 1 - train_f1_at_03:0.00440  valid_f1_at_03:0.00000\n","Epoch 1 - train_f1_at_05:0.00134  valid_f1_at_05:0.00000\n",">>>>>>>> Model Improved From -inf ----> 0.0\n","other scores here... 0.0, 0.0\n","Starting 2 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:14<00:00,  1.23s/it, loss=0.00843]\n","100%|| 93/93 [01:01<00:00,  1.51it/s, loss=0.00672]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.00843  avg_val_loss: 0.00672  time: 977s\n","Epoch 2 - train_f1_at_03:0.01216  valid_f1_at_03:0.17728\n","Epoch 2 - train_f1_at_05:0.00030  valid_f1_at_05:0.04007\n",">>>>>>>> Model Improved From 0.0 ----> 0.1772809981804003\n","other scores here... 0.1772809981804003, 0.04007071302298173\n","Starting 3 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:15<00:00,  1.23s/it, loss=0.00774]\n","100%|| 93/93 [01:00<00:00,  1.53it/s, loss=0.0061] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.00774  avg_val_loss: 0.00610  time: 977s\n","Epoch 3 - train_f1_at_03:0.04667  valid_f1_at_03:0.29352\n","Epoch 3 - train_f1_at_05:0.00210  valid_f1_at_05:0.10828\n",">>>>>>>> Model Improved From 0.1772809981804003 ----> 0.29352319706017455\n","other scores here... 0.29352319706017455, 0.1082766439909297\n","Starting 4 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:19<00:00,  1.24s/it, loss=0.00706]\n","100%|| 93/93 [01:01<00:00,  1.51it/s, loss=0.00591]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.00706  avg_val_loss: 0.00591  time: 982s\n","Epoch 4 - train_f1_at_03:0.11266  valid_f1_at_03:0.34775\n","Epoch 4 - train_f1_at_05:0.01057  valid_f1_at_05:0.13959\n",">>>>>>>> Model Improved From 0.29352319706017455 ----> 0.34774774774774775\n","other scores here... 0.34774774774774775, 0.13958682300390843\n","Starting 5 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:18<00:00,  1.24s/it, loss=0.00679]\n","100%|| 93/93 [01:01<00:00,  1.50it/s, loss=0.00502]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.00679  avg_val_loss: 0.00502  time: 981s\n","Epoch 5 - train_f1_at_03:0.15501  valid_f1_at_03:0.46235\n","Epoch 5 - train_f1_at_05:0.01483  valid_f1_at_05:0.25229\n",">>>>>>>> Model Improved From 0.34774774774774775 ----> 0.46235031459305875\n","other scores here... 0.46235031459305875, 0.2522899764459566\n","Starting 6 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:22<00:00,  1.24s/it, loss=0.00665]\n","100%|| 93/93 [01:02<00:00,  1.49it/s, loss=0.00459]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6 - avg_train_loss: 0.00665  avg_val_loss: 0.00459  time: 985s\n","Epoch 6 - train_f1_at_03:0.17986  valid_f1_at_03:0.53018\n","Epoch 6 - train_f1_at_05:0.02752  valid_f1_at_05:0.30508\n",">>>>>>>> Model Improved From 0.46235031459305875 ----> 0.530175706646295\n","other scores here... 0.530175706646295, 0.3050761421319797\n","Starting 7 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [17:14<00:00,  1.39s/it, loss=0.00637]\n","100%|| 93/93 [01:13<00:00,  1.26it/s, loss=0.00483]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7 - avg_train_loss: 0.00637  avg_val_loss: 0.00483  time: 1109s\n","Epoch 7 - train_f1_at_03:0.21463  valid_f1_at_03:0.50586\n","Epoch 7 - train_f1_at_05:0.03303  valid_f1_at_05:0.25894\n","Starting 8 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [17:46<00:00,  1.43s/it, loss=0.00606]\n","100%|| 93/93 [01:10<00:00,  1.32it/s, loss=0.00473]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8 - avg_train_loss: 0.00606  avg_val_loss: 0.00473  time: 1137s\n","Epoch 8 - train_f1_at_03:0.23858  valid_f1_at_03:0.51566\n","Epoch 8 - train_f1_at_05:0.04644  valid_f1_at_05:0.28058\n","Starting 9 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [17:44<00:00,  1.43s/it, loss=0.00606]\n","100%|| 93/93 [01:14<00:00,  1.25it/s, loss=0.00407]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9 - avg_train_loss: 0.00606  avg_val_loss: 0.00407  time: 1140s\n","Epoch 9 - train_f1_at_03:0.24388  valid_f1_at_03:0.58442\n","Epoch 9 - train_f1_at_05:0.04518  valid_f1_at_05:0.37460\n",">>>>>>>> Model Improved From 0.530175706646295 ----> 0.5844229675952246\n","other scores here... 0.5844229675952246, 0.3746047190464607\n","Starting 10 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [17:48<00:00,  1.44s/it, loss=0.006]  \n","100%|| 93/93 [01:11<00:00,  1.31it/s, loss=0.00394]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10 - avg_train_loss: 0.00600  avg_val_loss: 0.00394  time: 1141s\n","Epoch 10 - train_f1_at_03:0.26029  valid_f1_at_03:0.60662\n","Epoch 10 - train_f1_at_05:0.05535  valid_f1_at_05:0.38540\n",">>>>>>>> Model Improved From 0.5844229675952246 ----> 0.6066210467911965\n","other scores here... 0.6066210467911965, 0.3853965183752418\n","Starting 11 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:23<00:00,  1.32s/it, loss=0.00581]\n","100%|| 93/93 [01:09<00:00,  1.34it/s, loss=0.00446]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 11 - avg_train_loss: 0.00581  avg_val_loss: 0.00446  time: 1053s\n","Epoch 11 - train_f1_at_03:0.28853  valid_f1_at_03:0.55182\n","Epoch 11 - train_f1_at_05:0.06311  valid_f1_at_05:0.25714\n","Starting 12 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:05<00:00,  1.30s/it, loss=0.00556]\n","100%|| 93/93 [01:08<00:00,  1.35it/s, loss=0.00427]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 12 - avg_train_loss: 0.00556  avg_val_loss: 0.00427  time: 1035s\n","Epoch 12 - train_f1_at_03:0.32699  valid_f1_at_03:0.57127\n","Epoch 12 - train_f1_at_05:0.08093  valid_f1_at_05:0.30089\n","Starting 13 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:02<00:00,  1.30s/it, loss=0.0056] \n","100%|| 93/93 [01:08<00:00,  1.35it/s, loss=0.0037] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 13 - avg_train_loss: 0.00560  avg_val_loss: 0.00370  time: 1032s\n","Epoch 13 - train_f1_at_03:0.31917  valid_f1_at_03:0.64332\n","Epoch 13 - train_f1_at_05:0.08044  valid_f1_at_05:0.44367\n",">>>>>>>> Model Improved From 0.6066210467911965 ----> 0.6433189655172414\n","other scores here... 0.6433189655172414, 0.4436685288640596\n","Starting 14 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:05<00:00,  1.30s/it, loss=0.00557]\n","100%|| 93/93 [01:08<00:00,  1.36it/s, loss=0.0036] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 14 - avg_train_loss: 0.00557  avg_val_loss: 0.00360  time: 1035s\n","Epoch 14 - train_f1_at_03:0.32439  valid_f1_at_03:0.65698\n","Epoch 14 - train_f1_at_05:0.07994  valid_f1_at_05:0.44434\n",">>>>>>>> Model Improved From 0.6433189655172414 ----> 0.6569808646350107\n","other scores here... 0.6569808646350107, 0.4443409408476945\n","Starting 15 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [17:18<00:00,  1.40s/it, loss=0.00539]\n","100%|| 93/93 [01:17<00:00,  1.20it/s, loss=0.00408]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 15 - avg_train_loss: 0.00539  avg_val_loss: 0.00408  time: 1117s\n","Epoch 15 - train_f1_at_03:0.35093  valid_f1_at_03:0.60609\n","Epoch 15 - train_f1_at_05:0.09004  valid_f1_at_05:0.38556\n","Starting 16 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [18:12<00:00,  1.47s/it, loss=0.00515]\n","100%|| 93/93 [01:17<00:00,  1.19it/s, loss=0.00376]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 16 - avg_train_loss: 0.00515  avg_val_loss: 0.00376  time: 1171s\n","Epoch 16 - train_f1_at_03:0.37196  valid_f1_at_03:0.63888\n","Epoch 16 - train_f1_at_05:0.11505  valid_f1_at_05:0.43825\n","Starting 17 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [18:14<00:00,  1.47s/it, loss=0.00524]\n","100%|| 93/93 [01:20<00:00,  1.15it/s, loss=0.0034] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 17 - avg_train_loss: 0.00524  avg_val_loss: 0.00340  time: 1176s\n","Epoch 17 - train_f1_at_03:0.35697  valid_f1_at_03:0.68480\n","Epoch 17 - train_f1_at_05:0.10713  valid_f1_at_05:0.48736\n",">>>>>>>> Model Improved From 0.6569808646350107 ----> 0.6848033269797262\n","other scores here... 0.6848033269797262, 0.4873646209386282\n","Starting 18 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [17:39<00:00,  1.43s/it, loss=0.00521]\n","100%|| 93/93 [01:07<00:00,  1.37it/s, loss=0.00346]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 18 - avg_train_loss: 0.00521  avg_val_loss: 0.00346  time: 1128s\n","Epoch 18 - train_f1_at_03:0.36123  valid_f1_at_03:0.68039\n","Epoch 18 - train_f1_at_05:0.11120  valid_f1_at_05:0.45368\n","Starting 19 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:50<00:00,  1.36s/it, loss=0.00348]\n","100%|| 93/93 [01:14<00:00,  1.25it/s, loss=0.00418]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 19 - avg_train_loss: 0.00348  avg_val_loss: 0.00418  time: 1086s\n","Epoch 19 - train_f1_at_03:0.66943  valid_f1_at_03:0.61195\n","Epoch 19 - train_f1_at_05:0.46373  valid_f1_at_05:0.53254\n","Starting 20 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [16:25<00:00,  1.33s/it, loss=0.00328]\n","100%|| 93/93 [01:06<00:00,  1.40it/s, loss=0.00385]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 20 - avg_train_loss: 0.00328  avg_val_loss: 0.00385  time: 1052s\n","Epoch 20 - train_f1_at_03:0.69141  valid_f1_at_03:0.64950\n","Epoch 20 - train_f1_at_05:0.53349  valid_f1_at_05:0.56916\n","Starting 21 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:44<00:00,  1.27s/it, loss=0.00343]\n","100%|| 93/93 [01:05<00:00,  1.43it/s, loss=0.00342]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 21 - avg_train_loss: 0.00343  avg_val_loss: 0.00342  time: 1011s\n","Epoch 21 - train_f1_at_03:0.67658  valid_f1_at_03:0.70066\n","Epoch 21 - train_f1_at_05:0.51790  valid_f1_at_05:0.65307\n",">>>>>>>> Model Improved From 0.6848033269797262 ----> 0.7006622516556291\n","other scores here... 0.7006622516556291, 0.6530692292606394\n","Starting 22 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:47<00:00,  1.27s/it, loss=0.00331]\n","100%|| 93/93 [01:05<00:00,  1.41it/s, loss=0.00352]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 22 - avg_train_loss: 0.00331  avg_val_loss: 0.00352  time: 1014s\n","Epoch 22 - train_f1_at_03:0.69173  valid_f1_at_03:0.69120\n","Epoch 22 - train_f1_at_05:0.54102  valid_f1_at_05:0.63200\n","Starting 23 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:45<00:00,  1.27s/it, loss=0.00297]\n","100%|| 93/93 [01:05<00:00,  1.42it/s, loss=0.00429]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 23 - avg_train_loss: 0.00297  avg_val_loss: 0.00429  time: 1012s\n","Epoch 23 - train_f1_at_03:0.72761  valid_f1_at_03:0.65045\n","Epoch 23 - train_f1_at_05:0.58902  valid_f1_at_05:0.60307\n","Starting 24 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:45<00:00,  1.27s/it, loss=0.00285]\n","100%|| 93/93 [01:05<00:00,  1.42it/s, loss=0.00372]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 24 - avg_train_loss: 0.00285  avg_val_loss: 0.00372  time: 1012s\n","Epoch 24 - train_f1_at_03:0.73849  valid_f1_at_03:0.68645\n","Epoch 24 - train_f1_at_05:0.61157  valid_f1_at_05:0.65474\n","Starting 25 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [17:09<00:00,  1.39s/it, loss=0.00305]\n","100%|| 93/93 [01:15<00:00,  1.24it/s, loss=0.00342]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 25 - avg_train_loss: 0.00305  avg_val_loss: 0.00342  time: 1105s\n","Epoch 25 - train_f1_at_03:0.71744  valid_f1_at_03:0.71291\n","Epoch 25 - train_f1_at_05:0.58369  valid_f1_at_05:0.67183\n",">>>>>>>> Model Improved From 0.7006622516556291 ----> 0.7129075182967398\n","other scores here... 0.7129075182967398, 0.6718296224588576\n","Starting 26 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [17:38<00:00,  1.42s/it, loss=0.00292]\n","100%|| 93/93 [01:10<00:00,  1.31it/s, loss=0.00373]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 26 - avg_train_loss: 0.00292  avg_val_loss: 0.00373  time: 1130s\n","Epoch 26 - train_f1_at_03:0.73392  valid_f1_at_03:0.70540\n","Epoch 26 - train_f1_at_05:0.60385  valid_f1_at_05:0.66357\n","Starting 27 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:52<00:00,  1.28s/it, loss=0.00261]\n","100%|| 93/93 [01:05<00:00,  1.41it/s, loss=0.00405]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 27 - avg_train_loss: 0.00261  avg_val_loss: 0.00405  time: 1019s\n","Epoch 27 - train_f1_at_03:0.76214  valid_f1_at_03:0.67263\n","Epoch 27 - train_f1_at_05:0.65092  valid_f1_at_05:0.63114\n","Starting 28 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:40<00:00,  1.27s/it, loss=0.00261]\n","100%|| 93/93 [01:05<00:00,  1.43it/s, loss=0.00361]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 28 - avg_train_loss: 0.00261  avg_val_loss: 0.00361  time: 1006s\n","Epoch 28 - train_f1_at_03:0.76550  valid_f1_at_03:0.69917\n","Epoch 28 - train_f1_at_05:0.64478  valid_f1_at_05:0.66241\n","Starting 29 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:45<00:00,  1.27s/it, loss=0.00275]\n","100%|| 93/93 [01:05<00:00,  1.42it/s, loss=0.0035] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 29 - avg_train_loss: 0.00275  avg_val_loss: 0.00350  time: 1011s\n","Epoch 29 - train_f1_at_03:0.75082  valid_f1_at_03:0.71344\n","Epoch 29 - train_f1_at_05:0.62907  valid_f1_at_05:0.68809\n",">>>>>>>> Model Improved From 0.7129075182967398 ----> 0.7134367778144602\n","other scores here... 0.7134367778144602, 0.6880943324457969\n","Starting 30 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [15:42<00:00,  1.27s/it, loss=0.00255]\n","100%|| 93/93 [01:06<00:00,  1.40it/s, loss=0.00378]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 30 - avg_train_loss: 0.00255  avg_val_loss: 0.00378  time: 1010s\n","Epoch 30 - train_f1_at_03:0.77334  valid_f1_at_03:0.70076\n","Epoch 30 - train_f1_at_05:0.66477  valid_f1_at_05:0.67342\n","Starting 31 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [17:44<00:00,  1.43s/it, loss=0.00226]\n","100%|| 93/93 [01:16<00:00,  1.22it/s, loss=0.00431]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 31 - avg_train_loss: 0.00226  avg_val_loss: 0.00431  time: 1142s\n","Epoch 31 - train_f1_at_03:0.79939  valid_f1_at_03:0.67019\n","Epoch 31 - train_f1_at_05:0.69803  valid_f1_at_05:0.62830\n","Starting 32 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 743/743 [17:41<00:00,  1.43s/it, loss=0.00234]\n","100%|| 93/93 [01:11<00:00,  1.30it/s, loss=0.00377]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 32 - avg_train_loss: 0.00234  avg_val_loss: 0.00377  time: 1133s\n","Epoch 32 - train_f1_at_03:0.79309  valid_f1_at_03:0.71752\n","Epoch 32 - train_f1_at_05:0.68913  valid_f1_at_05:0.69224\n",">>>>>>>> Model Improved From 0.7134367778144602 ----> 0.7175169225689284\n","other scores here... 0.7175169225689284, 0.6922353825907125\n","====================================================================================================\n","Fold 2 Training\n","====================================================================================================\n"]},{"name":"stderr","output_type":"stream","text":["/home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Starting 1 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["  2%|         | 12/743 [00:17<17:21,  1.43s/it, loss=0.302]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000018?line=38'>39</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000018?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m<\u001b[39m CFG\u001b[39m.\u001b[39mcutmix_and_mixup_epochs:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000018?line=41'>42</a>\u001b[0m     train_avg, train_loss \u001b[39m=\u001b[39m train_mixup_cutmix_fn(model, train_dataloader, device, optimizer, scheduler)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000018?line=42'>43</a>\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000018?line=43'>44</a>\u001b[0m     train_avg, train_loss \u001b[39m=\u001b[39m train_fn(model, train_dataloader, device, optimizer, scheduler)\n","\u001b[1;32m/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb Cell 17'\u001b[0m in \u001b[0;36mtrain_mixup_cutmix_fn\u001b[0;34m(model, data_loader, device, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000016?line=30'>31</a>\u001b[0m scores \u001b[39m=\u001b[39m MetricMeter()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000016?line=31'>32</a>\u001b[0m tk0 \u001b[39m=\u001b[39m tqdm(data_loader, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data_loader))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000016?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tk0:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000016?line=34'>35</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000016?line=35'>36</a>\u001b[0m     inputs \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n","File \u001b[0;32m~/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/tqdm/std.py?line=1191'>1192</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/tqdm/std.py?line=1193'>1194</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/tqdm/std.py?line=1194'>1195</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/tqdm/std.py?line=1195'>1196</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/tqdm/std.py?line=1196'>1197</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/tqdm/std.py?line=1197'>1198</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n","File \u001b[0;32m~/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m~/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","\u001b[1;32m/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb Cell 14'\u001b[0m in \u001b[0;36mWaveformDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=158'>159</a>\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([y, y, y])[:AudioParams\u001b[39m.\u001b[39mduration \u001b[39m*\u001b[39m AudioParams\u001b[39m.\u001b[39msr] \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=159'>160</a>\u001b[0m y \u001b[39m=\u001b[39m crop_or_pad(y, AudioParams\u001b[39m.\u001b[39mduration \u001b[39m*\u001b[39m AudioParams\u001b[39m.\u001b[39msr, sr\u001b[39m=\u001b[39mAudioParams\u001b[39m.\u001b[39msr, train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, probs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=160'>161</a>\u001b[0m image \u001b[39m=\u001b[39m compute_melspec(y, AudioParams)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=161'>162</a>\u001b[0m image \u001b[39m=\u001b[39m mono_to_color(image)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=162'>163</a>\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n","\u001b[1;32m/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb Cell 14'\u001b[0m in \u001b[0;36mcompute_melspec\u001b[0;34m(y, params)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_melspec\u001b[39m(y, params):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=1'>2</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=2'>3</a>\u001b[0m \u001b[39m    Computes a mel-spectrogram and puts it at decibel scale\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=3'>4</a>\u001b[0m \u001b[39m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=7'>8</a>\u001b[0m \u001b[39m        np array -- Mel-spectrogram\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=8'>9</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=9'>10</a>\u001b[0m     melspec \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mfeature\u001b[39m.\u001b[39;49mmelspectrogram(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=10'>11</a>\u001b[0m         y\u001b[39m=\u001b[39;49my, sr\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49msr, n_mels\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mn_mels, fmin\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mfmin, fmax\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mfmax,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=11'>12</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=13'>14</a>\u001b[0m     melspec \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mpower_to_db(melspec)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/olly/Documents/Kaggle_BirdCLEF_2022/birdclef2022-use-2nd-label-train_V3.ipynb#ch0000013?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m melspec\n","File \u001b[0;32m~/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/librosa/util/decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/librosa/util/decorators.py?line=85'>86</a>\u001b[0m extra_args \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(all_args)\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/librosa/util/decorators.py?line=86'>87</a>\u001b[0m \u001b[39mif\u001b[39;00m extra_args \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/librosa/util/decorators.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/librosa/util/decorators.py?line=89'>90</a>\u001b[0m \u001b[39m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/librosa/util/decorators.py?line=90'>91</a>\u001b[0m args_msg \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/librosa/util/decorators.py?line=91'>92</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name, arg)\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/librosa/util/decorators.py?line=92'>93</a>\u001b[0m     \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[39m-\u001b[39mextra_args:])\n\u001b[1;32m     <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/librosa/util/decorators.py?line=93'>94</a>\u001b[0m ]\n","File \u001b[0;32m~/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/librosa/feature/spectral.py:2058\u001b[0m, in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/librosa/feature/spectral.py?line=2054'>2055</a>\u001b[0m \u001b[39m# Build a Mel filter\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/librosa/feature/spectral.py?line=2055'>2056</a>\u001b[0m mel_basis \u001b[39m=\u001b[39m filters\u001b[39m.\u001b[39mmel(sr\u001b[39m=\u001b[39msr, n_fft\u001b[39m=\u001b[39mn_fft, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/librosa/feature/spectral.py?line=2057'>2058</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49meinsum(\u001b[39m\"\u001b[39;49m\u001b[39m...ft,mf->...mt\u001b[39;49m\u001b[39m\"\u001b[39;49m, S, mel_basis, optimize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n","File \u001b[0;32m~/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/numpy/core/einsumfunc.py:1407\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/numpy/core/einsumfunc.py?line=1403'>1404</a>\u001b[0m     right_pos\u001b[39m.\u001b[39mappend(input_right\u001b[39m.\u001b[39mfind(s))\n\u001b[1;32m   <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/numpy/core/einsumfunc.py?line=1405'>1406</a>\u001b[0m \u001b[39m# Contract!\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/numpy/core/einsumfunc.py?line=1406'>1407</a>\u001b[0m new_view \u001b[39m=\u001b[39m tensordot(\u001b[39m*\u001b[39;49mtmp_operands, axes\u001b[39m=\u001b[39;49m(\u001b[39mtuple\u001b[39;49m(left_pos), \u001b[39mtuple\u001b[39;49m(right_pos)))\n\u001b[1;32m   <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/numpy/core/einsumfunc.py?line=1408'>1409</a>\u001b[0m \u001b[39m# Build a new view if needed\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/numpy/core/einsumfunc.py?line=1409'>1410</a>\u001b[0m \u001b[39mif\u001b[39;00m (tensor_result \u001b[39m!=\u001b[39m results_index) \u001b[39mor\u001b[39;00m handle_out:\n","File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","File \u001b[0;32m~/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/numpy/core/numeric.py:1132\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes)\u001b[0m\n\u001b[1;32m   <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/numpy/core/numeric.py?line=1129'>1130</a>\u001b[0m at \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mtranspose(newaxes_a)\u001b[39m.\u001b[39mreshape(newshape_a)\n\u001b[1;32m   <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/numpy/core/numeric.py?line=1130'>1131</a>\u001b[0m bt \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mtranspose(newaxes_b)\u001b[39m.\u001b[39mreshape(newshape_b)\n\u001b[0;32m-> <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/numpy/core/numeric.py?line=1131'>1132</a>\u001b[0m res \u001b[39m=\u001b[39m dot(at, bt)\n\u001b[1;32m   <a href='file:///home/olly/miniconda3/envs/AudioML_3060/lib/python3.9/site-packages/numpy/core/numeric.py?line=1132'>1133</a>\u001b[0m \u001b[39mreturn\u001b[39;00m res\u001b[39m.\u001b[39mreshape(olda \u001b[39m+\u001b[39m oldb)\n","File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# main loop\n","for fold in [4,3,2,1,0]:   #range(5):\n","    if fold not in CFG.folds:\n","        continue\n","    print(\"=\" * 100)\n","    print(f\"Fold {fold} Training\")\n","    print(\"=\" * 100)\n","\n","    trn_df = train[train.kfold != fold].reset_index(drop=True)\n","    val_df = train[train.kfold == fold].reset_index(drop=True)\n","\n","    train_dataset = WaveformDataset(df=trn_df, mode='train')\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=CFG.train_bs, num_workers=0, pin_memory=True, shuffle=True\n","    )\n","    \n","    valid_dataset = WaveformDataset(df=val_df, mode='valid')\n","    valid_dataloader = torch.utils.data.DataLoader(\n","        valid_dataset, batch_size=CFG.valid_bs, num_workers=0, pin_memory=True, shuffle=False\n","    )\n","\n","    model = TimmSED(\n","        base_model_name=CFG.base_model_name,\n","        pretrained=CFG.pretrained,\n","        num_classes=CFG.num_classes,\n","        in_channels=CFG.in_channels)\n","\n","    optimizer = transformers.AdamW(model.parameters(), lr=CFG.LR, weight_decay=CFG.WEIGHT_DECAY)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=CFG.ETA_MIN, T_max=500)\n","\n","    model = model.to(device)\n","\n","    min_loss = 999\n","    best_score = -np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        print(\"Starting {} epoch...\".format(epoch+1))\n","\n","        start_time = time.time()\n","\n","        if epoch < CFG.cutmix_and_mixup_epochs:\n","            train_avg, train_loss = train_mixup_cutmix_fn(model, train_dataloader, device, optimizer, scheduler)\n","        else: \n","            train_avg, train_loss = train_fn(model, train_dataloader, device, optimizer, scheduler)\n","\n","        valid_avg, valid_loss = valid_fn(model, valid_dataloader, device)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f'Epoch {epoch+1} - avg_train_loss: {train_loss:.5f}  avg_val_loss: {valid_loss:.5f}  time: {elapsed:.0f}s')\n","        print(f\"Epoch {epoch+1} - train_f1_at_03:{train_avg['f1_at_03']:0.5f}  valid_f1_at_03:{valid_avg['f1_at_03']:0.5f}\")\n","        print(f\"Epoch {epoch+1} - train_f1_at_05:{train_avg['f1_at_05']:0.5f}  valid_f1_at_05:{valid_avg['f1_at_05']:0.5f}\")\n","\n","        if valid_avg['f1_at_03'] > best_score:\n","            print(f\">>>>>>>> Model Improved From {best_score} ----> {valid_avg['f1_at_03']}\")\n","            print(f\"other scores here... {valid_avg['f1_at_03']}, {valid_avg['f1_at_05']}\")\n","            torch.save(model.state_dict(), f'fold-{fold}.bin')\n","            best_score = valid_avg['f1_at_03']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_paths = [f'fold-{i}.bin' for i in CFG.folds]\n","\n","calc_cv(model_paths)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"ccd939c93ab068b4b58227f04d922ef82bb8e88e5b6f5613cfc27fa317c2006f"},"kernelspec":{"display_name":"Python 3.9.1 ('AudioML_3060')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":4}
