{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Inference Notebook for BirdCLEF2022 ##\n\n[My fork](https://www.kaggle.com/code/ollypowell/birdclef2022-infer)  \n[Forked Kaerururu](https://www.kaggle.com/code/kaerunantoka/birdclef2022-ex005-f0-infer/notebook)  \n[Original baseline by Kaerururu](https://www.kaggle.com/code/kaerunantoka/birdclef2022-use-2nd-label-f0/notebook)  \n[That was forked from Kidehisa Arai (2021 comp)](https://www.kaggle.com/code/hidehisaarai1213/pytorch-inference-birdclef2021-starter/notebook)  \n[Original training notebook by Kaerururu](https://www.kaggle.com/code/kaerunantoka/birdclef2022-use-2nd-label-f0)\n\nData:  \nhttps://www.kaggle.com/kaerunantoka/birdclef2022-audio-to-numpy-1-4  \nhttps://www.kaggle.com/kaerunantoka/birdclef2022-audio-to-numpy-2-4  \nhttps://www.kaggle.com/kaerunantoka/birdclef2022-audio-to-numpy-3-4  \nhttps://www.kaggle.com/kaerunantoka/birdclef2022-audio-to-numpy-4-4  \n\n\n**My Strategy:**  \n1. Set up the training notebooks on my own GPUs, so no time limits\n2. Rund the audio to numpy code on each machine, to convert the ogg files to numpy (makes really big files!)\n3. Run with all folds, while I work on next step\n4. Clean up the redundant libraries, data in this notebook, and get inference running with a larger batch size for speed\n5. Run inference on Kaggle once the models are ready\n6. Improve my CV score as much as I can on my faster machine by doing some additional data augmentation, try larger image size\n7. Fine tune the inference threshold (Should be in the vacinity of .005 to 0.1)\n8. Try and think of some post-processing strategies using the combination of endemic birds/locations involved\n\n\n[**Basic Augmentation strategies, suggested to be useful by Shinmaru:**](https://www.kaggle.com/competitions/birdclef-2022/discussion/324318)\n\n* Time shift\n* Add pink noise and brown noise   (pink noise already used in the training notebook)\n* Mix other audio dataset\n\n[Good notebook on this topic by Hidehisa Arai](https://www.kaggle.com/code/hidehisaarai1213/rfcx-audio-data-augmentation-japanese-english)  \n[Time and noise only covered by Shinmaru](https://www.kaggle.com/code/shinmurashinmura/birdclef2022-basic-augmentation/notebook)\n\n[**More advanced ideas (also suggested to work from Shinmaru)**](https://www.kaggle.com/competitions/birdclef-2022/discussion/307880)  \n\n[SpecAugment](https://arxiv.org/abs/1904.08779)  \n[SpecAugment++](https://arxiv.org/abs/2103.16858v3)  \n[ImportantAug](https://arxiv.org/abs/2112.07156)","metadata":{}},{"cell_type":"code","source":"!pip install ../input/torchlibrosa/torchlibrosa-0.0.5-py3-none-any.whl > /dev/null","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:37:52.046156Z","iopub.execute_input":"2022-05-24T19:37:52.046414Z","iopub.status.idle":"2022-05-24T19:38:20.791775Z","shell.execute_reply.started":"2022-05-24T19:37:52.046339Z","shell.execute_reply":"2022-05-24T19:38:20.79092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nimport os\nimport sys\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')    #not needed?\nimport random\nimport time\nimport warnings\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport soundfile as sf\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as torchdata\n\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import List\nfrom typing import Optional\n\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\nfrom torchlibrosa.stft import LogmelFilterBank, Spectrogram\nfrom torchlibrosa.augmentation import SpecAugmentation\nfrom tqdm import tqdm\n\nimport albumentations as A\nimport albumentations.pytorch.transforms as T\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:20.794161Z","iopub.execute_input":"2022-05-24T19:38:20.794716Z","iopub.status.idle":"2022-05-24T19:38:27.590434Z","shell.execute_reply.started":"2022-05-24T19:38:20.794673Z","shell.execute_reply":"2022-05-24T19:38:27.589687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = True  # type: ignore\n    torch.backends.cudnn.benchmark = True  # type: ignore\n    \n    \ndef get_logger(out_file=None):\n    logger = logging.getLogger()\n    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n    logger.handlers = []\n    logger.setLevel(logging.INFO)\n\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    handler.setLevel(logging.INFO)\n    logger.addHandler(handler)\n\n    if out_file is not None:\n        fh = logging.FileHandler(out_file)\n        fh.setFormatter(formatter)\n        fh.setLevel(logging.INFO)\n        logger.addHandler(fh)\n    logger.info(\"logger set up\")\n    return logger\n    \n    \n@contextmanager\ndef timer(name: str, logger: Optional[logging.Logger] = None):\n    t0 = time.time()\n    msg = f\"[{name}] start\"\n    if logger is None:\n        print(msg)\n    else:\n        logger.info(msg)\n    yield\n\n    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n    if logger is None:\n        print(msg)\n    else:\n        logger.info(msg)\n\n\ndef get_device() -> torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndevice = get_device()\nlogger = get_logger(\"main.log\")\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:27.593685Z","iopub.execute_input":"2022-05-24T19:38:27.593895Z","iopub.status.idle":"2022-05-24T19:38:27.669422Z","shell.execute_reply.started":"2022-05-24T19:38:27.59387Z","shell.execute_reply":"2022-05-24T19:38:27.668676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_layer(layer):\n    nn.init.xavier_uniform_(layer.weight)\n\n    if hasattr(layer, \"bias\"):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)\n\n\ndef init_bn(bn):\n    bn.bias.data.fill_(0.)\n    bn.weight.data.fill_(1.0)\n\n\ndef init_weights(model):\n    classname = model.__class__.__name__\n    if classname.find(\"Conv2d\") != -1:\n        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n        model.bias.data.fill_(0)\n    elif classname.find(\"BatchNorm\") != -1:\n        model.weight.data.normal_(1.0, 0.02)\n        model.bias.data.fill_(0)\n    elif classname.find(\"GRU\") != -1:\n        for weight in model.parameters():\n            if len(weight.size()) > 1:\n                nn.init.orghogonal_(weight.data)\n    elif classname.find(\"Linear\") != -1:\n        model.weight.data.normal_(0, 0.01)\n        model.bias.data.zero_()\n\n\ndef interpolate(x: torch.Tensor, ratio: int):\n    \"\"\"Interpolate data in time domain. This is used to compensate the\n    resolution reduction in downsampling of a CNN.\n    Args:\n      x: (batch_size, time_steps, classes_num)\n      ratio: int, ratio to interpolate\n    Returns:\n      upsampled: (batch_size, time_steps * ratio, classes_num)\n    \"\"\"\n    (batch_size, time_steps, classes_num) = x.shape\n    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n    return upsampled\n\n\ndef pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n    is the same as the value of the last frame.\n    Args:\n      framewise_output: (batch_size, frames_num, classes_num)\n      frames_num: int, number of frames to pad\n    Outputs:\n      output: (batch_size, frames_num, classes_num)\n    \"\"\"\n    output = F.interpolate(\n        framewise_output.unsqueeze(1),\n        size=(frames_num, framewise_output.size(2)),\n        align_corners=True,\n        mode=\"bilinear\").squeeze(1)\n\n    return output\n\n\n\nclass AttBlockV2(nn.Module):\n    def __init__(self,\n                 in_features: int,\n                 out_features: int,\n                 activation=\"linear\"):\n        super().__init__()\n\n        self.activation = activation\n        self.att = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True)\n        self.cla = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True)\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_layer(self.att)\n        init_layer(self.cla)\n\n    def forward(self, x):\n        # x: (n_samples, n_in, n_time)\n        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n        cla = self.nonlinear_transform(self.cla(x))\n        x = torch.sum(norm_att * cla, dim=2)\n        return x, norm_att, cla\n\n    def nonlinear_transform(self, x):\n        if self.activation == 'linear':\n            return x\n        elif self.activation == 'sigmoid':\n            return torch.sigmoid(x)\n\n\nclass TimmSED(nn.Module):\n    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1):\n        super().__init__()\n\n        self.spec_augmenter = SpecAugmentation(time_drop_width=64//2, time_stripes_num=2,\n                                               freq_drop_width=8//2, freq_stripes_num=2)\n\n        self.bn0 = nn.BatchNorm2d(CFG.n_mels)\n\n        base_model = timm.create_model(\n            base_model_name, pretrained=pretrained, in_chans=in_channels)\n        layers = list(base_model.children())[:-2]\n        self.encoder = nn.Sequential(*layers)\n\n        if hasattr(base_model, \"fc\"):\n            in_features = base_model.fc.in_features\n        else:\n            in_features = base_model.classifier.in_features\n\n        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n        self.att_block = AttBlockV2(\n            in_features, num_classes, activation=\"sigmoid\")\n\n        self.init_weight()\n\n    def init_weight(self):\n        init_bn(self.bn0)\n        init_layer(self.fc1)\n        \n\n    def forward(self, input_data):\n        x = input_data # (batch_size, 3, time_steps, mel_bins)\n\n        frames_num = x.shape[2]\n\n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)\n\n        if self.training:\n            if random.random() < 0.25:\n                x = self.spec_augmenter(x)\n\n        x = x.transpose(2, 3)\n\n        x = self.encoder(x)\n        \n        # Aggregate in frequency axis\n        x = torch.mean(x, dim=3)\n\n        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x = x1 + x2\n\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = x.transpose(1, 2)\n        x = F.relu_(self.fc1(x))\n        x = x.transpose(1, 2)\n        x = F.dropout(x, p=0.5, training=self.training)\n\n        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n        segmentwise_output = segmentwise_output.transpose(1, 2)\n\n        interpolate_ratio = frames_num // segmentwise_output.size(1)\n\n        # Get framewise output\n        framewise_output = interpolate(segmentwise_output,\n                                       interpolate_ratio)\n        framewise_output = pad_framewise_output(framewise_output, frames_num)\n\n        framewise_logit = interpolate(segmentwise_logit, interpolate_ratio)\n        framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n\n        output_dict = {\n            'framewise_output': framewise_output,\n            'clipwise_output': clipwise_output,\n            'logit': logit,\n            'framewise_logit': framewise_logit,\n        }\n\n        return output_dict","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:27.671903Z","iopub.execute_input":"2022-05-24T19:38:27.672292Z","iopub.status.idle":"2022-05-24T19:38:27.705278Z","shell.execute_reply.started":"2022-05-24T19:38:27.672252Z","shell.execute_reply":"2022-05-24T19:38:27.704529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = (0.485, 0.456, 0.406) # RGB\nstd = (0.229, 0.224, 0.225) # RGB\n\nalbu_transforms = {\n    'train' : A.Compose([\n            A.HorizontalFlip(p=0.5),\n            A.OneOf([\n                A.Cutout(max_h_size=5, max_w_size=16),\n                A.CoarseDropout(max_holes=4),\n            ], p=0.5),\n            A.Normalize(mean, std),\n    ]),\n    'valid' : A.Compose([\n            A.Normalize(mean, std),\n    ]),\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:27.707728Z","iopub.execute_input":"2022-05-24T19:38:27.708107Z","iopub.status.idle":"2022-05-24T19:38:27.721569Z","shell.execute_reply.started":"2022-05-24T19:38:27.708071Z","shell.execute_reply":"2022-05-24T19:38:27.720713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    EXP_ID = '018' \n\n    ######################\n    # Globals #\n    ######################\n    seed = 42\n    epochs = 5\n    train = True\n    folds = [0, 1, 2, 3, 4]\n    img_size = 224\n    main_metric = \"epoch_f1_at_03\"\n    minimize_metric = False\n\n    ######################\n    # Dataset #\n    ######################\n    transforms = {\n        \"train\": [{\"name\": \"Normalize\"}],\n        \"valid\": [{\"name\": \"Normalize\"}]\n    }\n    period = 5\n    n_mels = 224\n    fmin = 20\n    fmax = 16000\n    n_fft = 2048\n    hop_length = 512\n    sample_rate = 32000\n    melspectrogram_parameters = {\n        \"n_mels\": 224,\n        \"fmin\": 20,\n        \"fmax\": 16000\n    }\n\n    target_columns = 'afrsil1 akekee akepa1 akiapo akikik amewig aniani apapan arcter \\\n                      barpet bcnher belkin1 bkbplo bknsti bkwpet blkfra blknod bongul \\\n                      brant brnboo brnnod brnowl brtcur bubsan buffle bulpet burpar buwtea \\\n                      cacgoo1 calqua cangoo canvas caster1 categr chbsan chemun chukar cintea \\\n                      comgal1 commyn compea comsan comwax coopet crehon dunlin elepai ercfra eurwig \\\n                      fragul gadwal gamqua glwgul gnwtea golphe grbher3 grefri gresca gryfra gwfgoo \\\n                      hawama hawcoo hawcre hawgoo hawhaw hawpet1 hoomer houfin houspa hudgod iiwi incter1 \\\n                      jabwar japqua kalphe kauama laugul layalb lcspet leasan leater1 lessca lesyel lobdow lotjae \\\n                      madpet magpet1 mallar3 masboo mauala maupar merlin mitpar moudov norcar norhar2 normoc norpin \\\n                      norsho nutman oahama omao osprey pagplo palila parjae pecsan peflov perfal pibgre pomjae puaioh \\\n                      reccar redava redjun redpha1 refboo rempar rettro ribgul rinduc rinphe rocpig rorpar rudtur ruff \\\n                      saffin sander semplo sheowl shtsan skylar snogoo sooshe sooter1 sopsku1 sora spodov sposan \\\n                      towsol wantat1 warwhe1 wesmea wessan wetshe whfibi whiter whttro wiltur yebcar yefcan zebdov'.split()\n    \n    ######################\n    # Loaders #\n    ######################\n    loader_params = {\n        \"train\": {\n            \"batch_size\": 16, \n            \"num_workers\": 0,\n            \"shuffle\": True\n        },\n        \"valid\": {\n            \"batch_size\": 32,\n            \"num_workers\": 0,\n            \"shuffle\": False\n        }\n    }\n\n    ######################\n    # Model #\n    ######################\n    base_model_name = \"tf_efficientnet_b0_ns\"\n    pooling = \"max\"\n    pretrained = False\n    num_classes = 152\n    in_channels = 3\n\n    N_FOLDS = 5\n    LR = 1e-3\n    T_max=10\n    min_lr=1e-6","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:27.723077Z","iopub.execute_input":"2022-05-24T19:38:27.723538Z","iopub.status.idle":"2022-05-24T19:38:27.735068Z","shell.execute_reply.started":"2022-05-24T19:38:27.7235Z","shell.execute_reply":"2022-05-24T19:38:27.734307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUDIO_PATH = '../input/birdclef-2022/train_audio'\nCLASSES = sorted(os.listdir(AUDIO_PATH))\nNUM_CLASSES = len(CLASSES)\nclass AudioParams:\n    \"\"\"\n    Parameters used for the audio data\n    \"\"\"\n    sr = 32000\n    duration = 5\n    # Melspectrogram\n    n_mels = 224\n    fmin = 20\n    fmax = 16000","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:27.736325Z","iopub.execute_input":"2022-05-24T19:38:27.736791Z","iopub.status.idle":"2022-05-24T19:38:27.764847Z","shell.execute_reply.started":"2022-05-24T19:38:27.73676Z","shell.execute_reply":"2022-05-24T19:38:27.763999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_melspec(y, params):\n    \"\"\"\n    Computes a mel-spectrogram and puts it at decibel scale\n    Arguments:\n        y {np array} -- signal\n        params {AudioParams} -- Parameters to use for the spectrogram. Expected to have the attributes sr, n_mels, f_min, f_max\n    Returns:\n        np array -- Mel-spectrogram\n    \"\"\"\n    melspec = librosa.feature.melspectrogram(\n        y=y, sr=params.sr, n_mels=params.n_mels, fmin=params.fmin, fmax=params.fmax,\n    )\n\n    melspec = librosa.power_to_db(melspec).astype(np.float32)\n    return melspec\n\n\ndef mono_to_color(X, eps=1e-6, mean=None, std=None):\n    \"\"\"\n    Converts a one channel array to a 3 channel one in [0, 255]\n    Arguments:\n        X {numpy array [H x W]} -- 2D array to convert\n    Keyword Arguments:\n        eps {float} -- To avoid dividing by 0 (default: {1e-6})\n        mean {None or np array} -- Mean for normalization (default: {None})\n        std {None or np array} -- Std for normalization (default: {None})\n    Returns:\n        numpy array [3 x H x W] -- RGB numpy array\n    \"\"\"\n    X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    std = std or X.std()\n    X = (X - mean) / (std + eps)\n\n    # Normalize to [0, 255]\n    _min, _max = X.min(), X.max()\n\n    if (_max - _min) > eps:\n        V = np.clip(X, _min, _max)\n        V = 255 * (V - _min) / (_max - _min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(X, dtype=np.uint8)\n\n    return V","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:27.765941Z","iopub.execute_input":"2022-05-24T19:38:27.766244Z","iopub.status.idle":"2022-05-24T19:38:27.775787Z","shell.execute_reply.started":"2022-05-24T19:38:27.76621Z","shell.execute_reply":"2022-05-24T19:38:27.775112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(torchdata.Dataset):\n    def __init__(self, df: pd.DataFrame, clip: np.ndarray):\n        self.df = df\n        # self.clip = clip\n        self.clip = np.concatenate([clip, clip, clip])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx: int):\n        SR = 32000\n        sample = self.df.loc[idx, :]\n        row_id = sample.row_id\n\n        end_seconds = int(sample.seconds)\n        start_seconds = int(end_seconds - 5)\n        \n        # end_index = int(SR * (end_seconds + (self.train_period - 5) / 2) + len(self.clip) // 3)\n        # start_index = int(SR * (start_seconds - (self.train_period - 5) / 2) + len(self.clip) // 3)\n        \n        # y = self.clip[start_index:end_index].astype(np.float32)\n        image = self.clip[SR*start_seconds:SR*end_seconds].astype(np.float32)\n        image = np.nan_to_num(image)\n        \n        image = compute_melspec(image, AudioParams)\n        image = mono_to_color(image)\n        image = image.astype(np.uint8)\n\n        image = albu_transforms['valid'](image=image)['image'].T\n            \n        return {\n            \"image\": image,\n            \"row_id\": row_id,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:27.7781Z","iopub.execute_input":"2022-05-24T19:38:27.778311Z","iopub.status.idle":"2022-05-24T19:38:27.789215Z","shell.execute_reply.started":"2022-05-24T19:38:27.778286Z","shell.execute_reply":"2022-05-24T19:38:27.788414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_paths = ['../input/birdclef2022v3models/fold-0.bin','../input/birdclef2022v3models/fold-1.bin','../input/birdclef2022v3models/fold-2.bin','../input/birdclef2022v3models/fold-3.bin','../input/birdclef2022v3models/fold-4.bin']\n\nmodels = []\nfor p in model_paths:\n    model = TimmSED(\n        base_model_name=CFG.base_model_name,\n        pretrained=CFG.pretrained,\n        num_classes=CFG.num_classes,\n        in_channels=CFG.in_channels)\n    \n    model.to(device)\n    model.load_state_dict(torch.load(p))\n    model.eval()\n    models.append(model)\n    \nprint()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:27.792039Z","iopub.execute_input":"2022-05-24T19:38:27.792363Z","iopub.status.idle":"2022-05-24T19:38:33.589885Z","shell.execute_reply.started":"2022-05-24T19:38:27.792328Z","shell.execute_reply":"2022-05-24T19:38:33.589101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TARGET_SR = 32000\nDATADIR = Path(\"../input/birdclef-2022/test_soundscapes/\")","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:33.591266Z","iopub.execute_input":"2022-05-24T19:38:33.591752Z","iopub.status.idle":"2022-05-24T19:38:33.595866Z","shell.execute_reply.started":"2022-05-24T19:38:33.591714Z","shell.execute_reply":"2022-05-24T19:38:33.595154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_audios = list(DATADIR.glob(\"*.ogg\"))\nsample_submission = pd.read_csv('../input/birdclef-2022/sample_submission.csv')\nsample_submission\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:33.597915Z","iopub.execute_input":"2022-05-24T19:38:33.598316Z","iopub.status.idle":"2022-05-24T19:38:33.634045Z","shell.execute_reply.started":"2022-05-24T19:38:33.598283Z","shell.execute_reply":"2022-05-24T19:38:33.633155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction_for_clip(test_df: pd.DataFrame, \n                        clip: np.ndarray, \n                        models, \n                        threshold=0.05, \n                        threshold_long=None):\n\n    dataset = TestDataset(df=test_df, \n                          clip=clip,)\n    loader = torchdata.DataLoader(dataset, batch_size=1, shuffle=False)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n#     [model.eval() for model in models]\n    prediction_dict = {}\n    for data in tqdm(loader):\n        row_id = data['row_id'][0]\n        image = data['image'].to(device)\n\n        with torch.no_grad():\n            probas = []\n            probas_long = []\n            for model in models:\n                with torch.cuda.amp.autocast():\n                    output = model(image)\n                probas.append(output['clipwise_output'].detach().cpu().numpy().reshape(-1))\n                # probas_long.append(clipwise_pred_long.detach().cpu().numpy().reshape(-1))\n            probas = np.array(probas)\n            # probas_long = np.array(probas_long)\n#             probas = np.array([model(image)[1].detach().cpu().numpy().reshape(-1) for model in models])\n\n        #probas is a list of lists of probabilities for each model for a given segment of an audio clip\n        #[[p1m1, p2m1, p3m1...],[p1m2, p2m2, p3m2....].....]\n\n\n        if threshold_long is None:\n            events = probas.mean(0) >= threshold\n        else:\n            events = ((probas.mean(0) >= threshold).astype(int) \\\n                      + (probas_long.mean(0) >= threshold_long).astype(int)) >= 2\n        labels = np.argwhere(events).reshape(-1).tolist()\n#         labels = labels[:2]\n        if len(labels) == 0:\n            prediction_dict[str(row_id)] = \"nocall\"\n        else:\n            labels_str_list = list(map(lambda x: CFG.target_columns[x], labels))\n            label_string = \" \".join(labels_str_list)\n            prediction_dict[str(row_id)] = label_string\n    return prediction_dict     # Returns a dict for a single clip eg. {'soundscape_453028782_5': 'bird1 bird2 bird3', 'soundscape_453028782_10': 'bird2 bird3 bird6'...}","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:33.635594Z","iopub.execute_input":"2022-05-24T19:38:33.635856Z","iopub.status.idle":"2022-05-24T19:38:33.649564Z","shell.execute_reply.started":"2022-05-24T19:38:33.635823Z","shell.execute_reply":"2022-05-24T19:38:33.648832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction_for_clip_adjusted(test_df: pd.DataFrame,   #Currently not in use I don't know what's going wrong here, but it's only scoring 0.49, so no better than random results.\n                                clip: np.ndarray, \n                                models, \n                                threshold=0.05, \n                                threshold_long=None):\n\n    dataset = TestDataset(df=test_df, \n                          clip=clip,)\n    loader = torchdata.DataLoader(dataset, batch_size=1, shuffle=False)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    kauai_only = ['puaioh']\n    hawaii_only = ['akiapo', 'ercfra', 'elepai', 'hawcre', 'omao', 'skylar']\n    maui_only = ['crehon', 'hawpet1', 'maupar']\n    not_oahu = ['akiapo', 'aniani', 'barpet', 'crehon', 'elepai', 'ercfra', 'hawama', 'hawcre', 'hawgoo', 'hawhaw', 'hawpet1', 'iiwi', \n                'jabwar', 'maupar', 'omao', 'puaioh', 'skylar']\n    not_kauai = ['akiapo', 'apapan', 'barpet', 'crehon', 'elepai', 'ercfra', 'hawama', 'hawcre', 'hawgoo', 'hawhaw', 'hawpet1', \n                 'houfin', 'maupar', 'omao', 'skylar', 'yefkan']\n    not_hawaii = ['barpet', 'crehon', 'hawpet1', 'maupar', 'puaioh']\n    not_maui = ['akiapo', 'aniani', 'barpet', 'elepai', 'ercfra', 'hawcre', 'hawhaw', 'jabwar', 'omao', 'puaioh', 'skylar', 'yefcan']\n    all_birds = [3, 6, 7, 9, 44, 46, 47, 60, 62, 63, 64, 65, 67, 70, 72, 90, 101, 111, 131, 141, 150]  #The index of the 21 target birds in the full list\n    target_birds = [\"akiapo\", \"aniani\", \"apapan\", \"barpet\", \"crehon\", \"elepai\", \"ercfra\", \"hawama\", \"hawcre\", \"hawgoo\", \"hawhaw\", \n                 \"hawpet1\", \"houfin\", \"iiwi\", \"jabwar\", \"maupar\", \"omao\", \"puaioh\", \"skylar\", \"warwhe1\", \"yefcan\"]\n    \n#     [model.eval() for model in models]\n    prediction_dict = {}\n    audio_file_probas = []\n    for data in tqdm(loader):\n        row_id = data['row_id'][0]          # soundscape_453028782_5\n        image = data['image'].to(device)    # Image is a 5 second chunk?\n        \n        with torch.no_grad():\n            probas = []\n            probas_long = []\n            for model in models:\n                with torch.cuda.amp.autocast():\n                    output = model(image)\n                probas.append(output['clipwise_output'].detach().cpu().numpy().reshape(-1))\n            probas = np.array(probas)\n\n            #probas is an array of probabilities for each model for a given segment of an audio clip\n            #[[p1m1, p2m1, p3m1...],\n            # [p1m2, p2m2, p3m2....]\n            # [p1m3, p2m3, p3m3....].....]\n            \n        mean_probas=probas.mean(0)  # A 1d array [p1, p2, p3 ....]  Mean probability of each bird for that clip\n        \n        audio_file_probas.append(mean_probas) # Builds a list of lists of np 1D arrays of probabilities for each 5 second clip\n        #[[bird1_p1 bird2_p2 bird3_p3...][the same+5s][the same+10s]....]\n        \n    file_max_probas = np.array(audio_file_probas).max(0)  # Find the maximum value for each bird. \n    # A 1d array [p1 p2 p3 p4]\n    # Use this  to see if the most probable bird is endemic to a particular island\n    # List all the birds that are not on that island\n    # Remove those birds from the results list\n    \n    hawaii_probas=[]\n    for bird in all_birds:\n        hawaii_probas.append(file_max_probas[bird])\n    maxbird = target_birds[hawaii_probas.index(max(hawaii_probas))]\n\n    nobird_list = []\n    if maxbird in kauai_only:\n        for bird in not_kauai:\n            nobird_list.append(CFG.target_columns.index(bird)) \n    if maxbird in hawaii_only:\n        for bird in not_hawaii:\n            nobird_list.append(CFG.target_columns.index(bird)) \n    if maxbird in maui_only:\n        for bird in not_maui:\n            nobird_list.append(CFG.target_columns.index(bird)) \n        \n    for probas in audio_file_probas: \n        events = probas >= threshold \n        all_labels = np.argwhere(events).reshape(-1).tolist()   #argwhere returns the indices of non-zero elements (True elements in this case) \n\n        if len(all_labels) == 0:\n            prediction_dict[str(row_id)] = \"nocall\"\n        else:\n            labels = [x for x in all_labels if x not in nobird_list]  #remove the birds that can't be there\n            print(labels)\n            labels_str_list = list(map(lambda x: CFG.target_columns[x], labels))  # matches the indices to the column number in the bird name list\n            label_string = \" \".join(labels_str_list)\n            prediction_dict[str(row_id)] = label_string   \n    return prediction_dict     # Returns a dict for a single clip eg. {'soundscape_453028782_5': 'bird1 bird2 bird3', 'soundscape_453028782_10': 'bird2 bird3 bird6'...}","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:33.650772Z","iopub.execute_input":"2022-05-24T19:38:33.651333Z","iopub.status.idle":"2022-05-24T19:38:33.673939Z","shell.execute_reply.started":"2022-05-24T19:38:33.651281Z","shell.execute_reply":"2022-05-24T19:38:33.673108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction(test_audios,\n               threshold=0.05, \n               threshold_long=None):\n    \n    # models = [model]\n    warnings.filterwarnings(\"ignore\")\n    prediction_dicts = {}\n    for audio_path in test_audios:\n        with timer(f\"Loading {str(audio_path)}\", logger):\n            clip, _ = sf.read(audio_path, always_2d=True)\n            clip = np.mean(clip, 1)\n            \n        seconds = []\n        row_ids = []\n        for second in range(5, 65, 5):\n            row_id = \"_\".join(audio_path.name.split(\".\")[:-1]) + f\"_{second}\"\n            seconds.append(second)   # A list of seconds for a given audio clip\n            row_ids.append(row_id)   # A list of rowids, in this form :soundscape_453028782_5\n        print(row_ids)\n        test_df = pd.DataFrame({     # A pandas df, with seconds and corresponding rowid for each audio file\n            \"row_id\": row_ids,\n            \"seconds\": seconds\n        })\n        with timer(f\"Prediction on {audio_path}\", logger):\n            prediction_dict = prediction_for_clip(test_df,\n                                                  clip=clip,\n                                                  models=models,\n                                                  threshold=threshold, threshold_long=threshold_long)\n\n        prediction_dicts.update(prediction_dict)   # Concatenates each of the audio file results.\n    return prediction_dicts","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:33.675037Z","iopub.execute_input":"2022-05-24T19:38:33.675458Z","iopub.status.idle":"2022-05-24T19:38:33.6871Z","shell.execute_reply.started":"2022-05-24T19:38:33.675422Z","shell.execute_reply":"2022-05-24T19:38:33.686285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = 0.04\nthreshold_long = None # 0.01\n\nprediction_dicts = prediction(test_audios=all_audios,\n           threshold=threshold, \n           threshold_long=threshold_long)\nprint(prediction_dicts)\n\nfor i in range(len(sample_submission)):\n    sample = sample_submission.row_id[i]\n    key = sample.split(\"_\")[0] + \"_\" + sample.split(\"_\")[1] + \"_\" + sample.split(\"_\")[3]\n    target_bird = sample.split(\"_\")[2]\n    print(key, target_bird)\n    if key in prediction_dicts:\n        sample_submission.iat[i, 1] = (target_bird in prediction_dicts[key])\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:33.688223Z","iopub.execute_input":"2022-05-24T19:38:33.688578Z","iopub.status.idle":"2022-05-24T19:38:41.462937Z","shell.execute_reply.started":"2022-05-24T19:38:33.68854Z","shell.execute_reply":"2022-05-24T19:38:41.462168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"execution":{"iopub.status.busy":"2022-05-24T19:38:41.467186Z","iopub.execute_input":"2022-05-24T19:38:41.469309Z","iopub.status.idle":"2022-05-24T19:38:41.481423Z","shell.execute_reply.started":"2022-05-24T19:38:41.469262Z","shell.execute_reply":"2022-05-24T19:38:41.480308Z"},"trusted":true},"execution_count":null,"outputs":[]}]}